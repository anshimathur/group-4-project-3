Speaker 1  0:04  
It's very costly. Yeah, okay, cool. So where's the time now? Yep, we are two minutes over. So let's get started. You guys already, okay, here just to confirm that you will be taking the attendance for today. Is that correct? I'm sorry, did you say me? Yeah, I was asking you, are you going to be the one taking attendance for the class today? That's going to think Muhammad is, oh, Mohammed. Oh, okay, so, yeah, it's fine. Yep. Okay. And when we go to the second half of the class for the student activities, we'll have to do the breakout rooms, and Karen will be handling that, the creation of the breakout rooms. So Karen, if you can, while I talk during the first part of the class, if you can take a note of how many participants that we have here, and then we will talk during the break time, what I'm watching, ideal size. Yeah,

Speaker 2  1:10  
peaks. And I know it's about 45 students. Is that out?

Speaker 1  1:16  
I think we had something around that yesterday.

Speaker 2  1:19  
Well, I get to run that number, and if we miss someone when i i watched two, then when I actually opened the breakout rooms, and people go to them, and I see one or two stragglers, and then I assign them at the end of So, big deal. Okay,

Speaker 1  1:36  
oh, so let's get started. Um, you guys all see my screen? I have a blue screen showing right now, nothing on it. You see that? Okay, cool. Okay, so let's get started. Okay, so first of all, the structure of the class today will be very similar to yesterday's class, where we are going to be starting to basically talk about starting to develop some theoretical understanding of the different machine learning techniques, some of the so called traditional machine learning, and what are the different kind of problems that we can solve using machine learning. And then we will also touch a little bit upon some of the advanced techniques that you can use, such as neural network and so on and so forth, as during the class today I am going to show, give you a very quick overview of some of the very simple, commonly used, widely used algorithms. But this class, by no means is designed for a deep dive into the actual machine learning algorithms. But just to pick your interest, I'm going to show you a little bit of glimpse into three, four different algorithms. But if you see something on the slides that you cannot quite understand, no worries about it, because those classes will come eventually, right? I just wanted to show you a little bit ahead what's coming.

Unknown Speaker  3:00  
Okay, so let's get started.

Speaker 1  3:04  
Okay, so the class objective today, we are going to talk about what is called supervised and unsupervised machine learning. And now you see I kind of have switched to use the term ml, which is machine learning, instead of AI, right? Because that's why I was talking about yesterday. So towards the end of the correct this course will be probably doing something that you can say, Yes, we are doing some AI stuff. But as you start from the beginning, you will basically mostly do machine learning, right? It is fun. It's not it's not boring or anything. Machine learning is far and that's what I hope to kind of help you all see through before you actually jump into more advanced kind of sort of state of the art, AI stuff towards the better part of the right. So, AI, yeah, so machine learning. And then we will very briefly touch about, touch upon some of the complex AI techniques, some of the more modern techniques, including neural networks, deep learning and so on. And then in the second half of the class, we will switch gear, just like yesterday we talked about, we basically learned how to use the terminal, very basic. Today, we are going to learn how to use GitHub repository, how to create a repository, how to check in, check out files, how to commit and stuff like that, right? So the idea is that when the time comes, starting next week, when we actually, when you actually start to reduce your work, you should have no problem actually operating the GitHub and understand how the whole work works, right? What are the different commands, how you manage your files, the life cycle and everything. So that's what we are going to do in the second half of the class today. Okay, so let's get started. So machine learning algorithm. Sorry, machine learning definition, like we were discussing yesterday, it is the practice of applying algorithm and statistics to create models. Is that can learn from data and then make decision or prediction about the data that it has not seen future data. You can call it a future data or unseen data. So the way it works is the machine learning algorithm. It learns to make decision without you telling explicitly how to make the decision, how it does that. So the algorithm that we will be looking into it, what they do is they learn the patterns and behavior looking into the data directly, so you don't have to write conditional logic say, hey, if these they'll do that. That is your traditional programming, right? The conditional logic in machine learning, you don't do that. You simply feed the program with some data, and then data figures out which data belongs to which group or which data points supposed to come next after all of these data, which is what classification and regression was two

Unknown Speaker  6:03  
main category of machine learning, yes.

Speaker 1  6:06  
So talking about the types of machine learning, there are actually these are the two major types of machine learning, although these are not everything, but for your purpose to begin with, just know that there are two broad category of machine learning. One is called supervised learning. Supervised learning is something where you have to provide some help, some guidance to the model, to the machine learning model that you will be using, to help it understand the pattern. That's what supervision is. Supervision, by the way, by no means. Mean that you have to sit there and watch your program running and do stuff. So it's not supervision in that sense, supervision in that sense that you have to provide some example of data that you have looked through before you, meaning not you. I mean you can collect these type of data from somewhere else as well, which is what people do all the time anyway. But the point here is, you just don't give the data to the machine. You also provide some clue, some hint, like what this data point belongs to, and that's what we call supervised learning. So using supervised learning, you then understand you're not you the algorithm then understand the pattern, and then it can predict which data point belongs to which group, and so on and so forth. Unsupervised Learning, on the other hand, is just a collection of data where no human has ever touched the data. You have some data point, you feed the data point to a computer program to the machine learning algorithm, hoping that the computer program or the algorithm find some useful relations, some useful pattern within the data that helps you understand the data, the data landscape better. That's what supervised unsupervised learning is. Now you might have hard other star, other terms such as reinforced learning, which does not belong to any of these categories, which is not going to be our focus point, but just for the sake of completeness, I wanted to talk about reinforcement learning as well. So Reinforcement learning is basically kind of a learning mechanism which kind of mimics the way how, let's say kids learn, right? How babies learn, right? Teaching someone good behavior versus bad behavior, right? So let's say you are a new one. You are a baby, not not new born. Maybe you baby, a baby or like a toddler. Let's say right, if the toddler does something good, you give you reward you, right? If the toddler something does something that is not good, you kind of send some negative feedback, right? Kind of a characteristic approach. So you can do same thing with some of the machine learning algorithm as well, where you reward good behavior and punish bad behavior over time. The idea is that that program will learn which behavior is good which behavior is bad. And this is something, by the way, very widely used in training, things like self driving cars, right, autonomous robots, right, where there is an intelligent entity working, and then you want to teach the entity what to do, what not to do, like, Hey, this is the traffic rules. If you follow the traffic rule, you will be rewarded. If you don't follow, if you overspeed, if you jump a red light a stop sign, then you will be punished, right? So that's kind of the domain of reinforcement learning, which is kind of different from either of the supervised and unsupervised learning. So for now, forget about reinforcement learning. You can have that in the back of your mind, but for your purposes, just two classes have algorithm that you need to know. One is supervised and one is unsupervised learning.

Speaker 3  9:47  
Can now, within Can I ask a question, sure, can you give an example of when you would use supervised versus unsupervised learning? Because I'm trying, I'm having a hard time understanding that the.

Speaker 1  9:59  
So that's that's because you are thinking ahead, which is a good sign. That means you are thinking on the right track. So next few slides, we are going to talk about just that. Obviously, I'm not going to leave you just with the definition of two classes of learning. We are going to talk more about it. Okay, no problem. So unsupervised learning, there are two primary applications of unsupervised learning. So on the left side of the screen you see unsupervised learning, and the two application there, on the yellow circle, you see dimensionality reduction, and the blue circle you see clustering. And I'm going to explain what this means in a second, and the supervised learning, the two major application of supervised learning is classification and regression. Now, some of you might be thinking, Well, speak, explain that to me in English, like I don't know what dimensional reduction means, right? So raise your hand if you think that. Yep, that's what I was expected. A good percentage will raise their hand saying, what the heck on the world is dimensionality reduction? And in the slide here, I'd agree we really don't do much justice. All we are saying is, hey, this is used in big data visualization, feature discernment, structure discovery. It's still gibberish to me, at least right. Clustering, we use this for segmentation, recommended system, targeted marketing. So these are all some of the application. None of these tells what this actually means, and that's why I have next few slides where we are going to talk about each of these four classes of application, right? So before going there in general, what I was saying before In supervised learning, you have to know, sorry, in unsupervised learning. So we are first talking about unsupervised where you have a bunch of data points, and the data points do not have any level. So here, since if you look into this graphic, right? So what it is representing here is you have a bunch of different data points. Some are blue, some are like light blue, some are dark blue, some are yellow, right? Some are circle, some are square, some are triangle. But the reason we are saying unleveled is we are not telling that to the program. We are not telling that to the machine. We, as a human, when we look at the data, if the data is very simple, we know, okay, which data is which, even we would not know if the data is more complex, right? But since this is a very simple, three geometric shape with three different color, we know what each type of data is, what each of the objects are. But we are not leveling this. We are not telling, hey, this object, this yellow, red thing, is a circle. We are not telling that in the tradition in the approach, general approach of unsupervised learning, is you feed the data as is, without leveling the data, and then let the algorithm do the interpretation. It will process the data, and the output, hopefully is that it will be able to even though we didn't tell which is which, but it should be able to see, well, at least there are three different classes of things. So machine will think, Well, the human didn't tell me which one is circle, which one is triangle, which one is square, so I have no idea, but at least what I can see is there are three kind of thing. So I better put them, put them into three different buckets. That is the idea behind unsupervised learning. So we are not telling that. So this mix of objects being grouped neatly into three different groups, it's all machine doing all by itself, without anyone telling it out that is the general idea behind supervised learning. Daniel question,

Speaker 4  14:00  
sorry about that, but I is the algorithm and the processing not happening at the same time because you have a divided to two factions? Are they not happening?

Speaker 1  14:11  
Well, yeah, yeah, this might say, so algorithm basically means the program. So algorithm is the program that you are using, and this algorithm is something that you can write, which you most often would not, you would basically end up using some Python library. So algorithm is either a pro a program, either you are writing or someone else have written before that you are just using as a library that's your algorithm, and processing is basically when the algorithm is actually being executed. The fact that algorithm is taking the data and doing the processing is shown here. And after the processing is complete, the result will be this. That's, that's what the graphic says, right? Okay, so I told you, I'll, I'll give you some better view of what each of these May. Means, what is clustering? What is dimensionality reduction, classification and integration. So first, we are going to start with this piece, which is called clustering, right? So by the way, this picture, what you are seeing here, this is actually a very, very accurate representation of what a clustering algorithm does. So clustering basically means you have a cluster of data without anyone telling anyone telling anything to the machine, the machine itself is managed to put them into separate cluster. That's essentially what the clustering algorithm does. The most common algorithm that you will hear about and which you will actually try your hands on is called the k means clustering. So the little overview of this, as I said before, it teaches machine to use unleveled, unclassified data with no training input. The goal here is to divide the data set into groups based on how similar or how different they are, right? Like think about the diagram or the graphic in the previous page, right? So these things are similar. They all blue. They all look like triangle. But this, each of these things is very different from each of these things, which look circular and yellow in color. So that's what your clustering algorithm does, that divide data into groups based on how similar or how different they are, and how it does that the third bullet point to the left, and bear with me for a moment. I'm going to explain that what I mean uses Euclidean distance in an n dimensional space as a measure of similarity. What does that mean? Okay, so, so, first of all, Euclidean space, a Euclidean distance. I hope you all have done algebra for sure at some point in your life. May not be in a very near past, but very far away, you have done algebra, right? So, in algebra, we learned a little theoretical Pythagorean theorem. You remember that? So if you have, like, an x y plane, and if you have two points, x1, y1, and x2 y2 then the distance is basically square root of x1, minus x2, whole square plus y1, minus y2, whole square right? Which is what the Pythagorean distance? Well, Euclidean distance is just another name of saying Pythagorean distance, because these type of distance is basically this geometry is called Euclidean geometry, right? So it's basically just going back to your high high school algebra. Okay. Now you might think like, Well, wait a minute. How does that come into play here? Okay, so now think about it. So let's say if you have a data point that has, each data has two coordinates value, right? So let's say in an x y data point, a plane, each data is a point. So then you will have a point called One two, and that will be a one point on your x y graph, right? You can have one data that says minus one minus four. That means x is minus 1y, is minus four. So depending on these value of the two number, it is easily possible for you to put a point, put a data point, anywhere in the x, y plane, right? It's kind of the same way on on to measure the Geo coordinates on on a for a location on the planet, we use latitude and longitude right except on a spherical but idea is the same, where you use two numeric value to uniquely locate a data point in some space that could be your paper, that could be surface of a globe, or that could be a three dimensional space. In that case, you will have a x, y, z, to have a 3d space. Or you can extend that in your mind so beyond three dimension. It does not exist in physical world, right? Because we know that we live in a 3d world. But mathematics is not limited by the world that we live in. That is the beauty of mathematics, right? In your mind, you are not constrained by three dimension you can have. You can live in a 300 dimension space, for that matter. Well, how do you express a point in a 300 dimension space? Well, all you need is 300 coordinates, instead of two coordinates or three coordinates, right? If you do, do happen to live in a 300 dimensional space, hypothetically speaking, right? There is no such thing really exist in the real world now. So think about it this way. So these things, right? So they have different properties in this graphical so circle and yellow, square and blue, triangle and dark blue, right? So these are the different properties. So it is possible for me to just use two words, one is the color, one is the shape, and using the two words, I can uniquely identify what each of my data point looks like. So now think of these two words, the. Color and shape equivalent to your x and y. Now, when you think that way, now, suddenly, there is a way for you to mathematically space these objects into a two dimensional space. What happens if your objects were more complex than this? What happens instead of having a picture of three different geometric objects, if you have a picture of, let's say, 10 different species of animal, right now, suddenly you cannot do that anymore, because if you, let's say, have a picture which is, let's say, 480 by 640 pixel and you have a picture right now, you cannot run really say, hey, in this picture, I see this geometric shape of this color. You really cannot say that. But what you can do is this, 480 times 640 however, that large number, there would be right couple of 100,000 you can take the pixel values for each of these and put it in a corresponding like, let's say just taking a number. Let's say it's 100,000 total pixel you have in your picture. So you can think of each of the pixel values as a point in 100,000 dimensional space in the same way that it would be in a two dimensional space. So that's what I meant when I said an n dimensional space.

Unknown Speaker  21:26  
Now,

Speaker 1  21:29  
what would be the Euclidean distance in that space? Well, it is the same. If it is a 2d space, it is square root of x, square plus y square. If it is a 3d space, it is a square root of x, square plus y, squared plus z square. If you have 100 numbers, it would be square root of first number square plus second number square plus third number square and so on. Up to 100 numbers, even if you have a million numbers, it doesn't care, because it's a computer who is going to do all the calculation, not you or me. So essentially, that same Euclidean geometric concept that we learn in algebra kind of carries over to this arbitrary high dimensional space. And in machine learning, when you are presenting the data to the to the model, to the algorithm, the algorithm basically interprets the data this way, and then it automatically measures and that's what the program inside the algorithm is that, what is the distance between two points? And the interpretation of the distance is, if the two things are similar, they will be closer in this space. The more dissimilar things are, the further apart they will be in this n dimensional space, which I sometimes like to call it hyperspace. Sounds kind of a very sci fi, right? Hyperspace. So, so in that n dimensional hyperspace, the distance between the two points is a measure of how similar or how dissimilar those two data points are, and that's what these algorithm uses, and not just this algorithm, any other algorithm uses that same concept of Euclidean Euclidean distance in an n dimensional hyperspace.

Unknown Speaker  23:07  
I know that was a lot question. I'll take a pause here. Yeah, I was just wondering, how you spell Euclidean distance. What?

Unknown Speaker  23:19  
What about it? Like? How you spell it? So I can do more research.

Speaker 1  23:23  
It's on screen. The third bullet point here, really where uses Euclidean distance in n dimensional space. You guys are not seeing my screen. Yeah. Okay. Oh.

Speaker 5  23:42  
I have a question about the patterns that you have earlier. I know it's kind of interesting. You have like a triangle circle and blues, but does it also recognize within similarities, like, for example, a yellow circle versus like a purple circle or both circles, right? But when you start different variation, yeah,

Speaker 1  24:01  
that will just be another point in a n dimensional space. And the beauty of this algorithm is you don't have to tell which are, which color it is or which shape it is. The algorithm will find it out by itself.

Speaker 5  24:13  
So you can be as specific as you want, and then the machines are, you know, the logic, or the algorithm, in this case, will be able to print it to the lowest, the smallest upset. Then right,

Speaker 1  24:25  
I see, yeah, and then on the right side of the screen, I basically provided the actual steps of processing that the algorithm will follow. Now I'm going to read through it. I'm not going to go too deep into explaining if some of these kind of eludes your eludes you at did in this class. Don't worry about it. We'll come back and revisit this when the time comes. But at a high level, what it does is, I'm going to try to explain it plain English as much as possible, and then I'm going to show you a little animation in the next slide, hopefully that will help you connect the dot. Better in your mind. Okay? So what it does is it assigned data points to one of the k clusters. So k basically means any number, right? And that's why it's called, oops, I'm getting ahead of myself, and that's why it's called k means clustering. So going back to the previous slide, so here, the value of k is 3k. Is an arbitrary number. Now this is something where, even though we are calling it unsupervised learning, but the machine might need a little bit of supervision from you as a human, if you happen to know, like, hey, how many possible clusters could there be? Then this very simplistic example, you can just easily look at the data and say, hey, yeah, I can see the k should be three for this. It is not so obvious in real world, world cases, right? So then you have to kind of use your judgment, but provide something like, Hey, try doing try doing this clustering with k equal to three or k equal to four or k equal to five, and then see what works better, right? But the computer does not know anything about the real world, so however many K you give it, give the program, it will create that many clusters. But those cluster may or may not be meaningful, depending on your underlying pattern of the data. So even though I said it is unsupervised, because you don't need leveling of the data, but the program still needs little bit of supervision, right? But then supervision also is something that you may not be able to provide in the first place, because, let's say, instead of trying to identify this. Let's think about another example. So let's say you have a customer database, right where you are storing data about customers behavior, and based on that customer behavior, like, which store do they buy? Do they go? What time of day? What is the customer zip code? What? What kind of product the customer buys, what is the average dollar amount the customer purchases, and so on. So random data about customer behavior, right? And then, if you want to identify, Okay, how many different kind of customer usually comes to the door in the hope of finding out, like, Okay, what kind of marketing programs that I need to do targeting these different demographic of customers without knowing what is the right so then it becomes bit of a trial and error, because you don't even know what the right case should be. So you have a whole bunch of data, couple of 1000 millions rows of data, but you don't know how many different groups is ideal. So then you it becomes kind of a trial and error. Well, you might talk to your marketing coordinator, what do you think? Yeah, I think baby boomers is one. I think that Gen Z. Gen Z is one, demography and so on, right? So you can go as fine grain or as coarse grain as you want, depending on the kind of business problem you're trying to solve. So by doing so, you come up with a number k, and then you feed that number K to the program, and then program works with that k value, right? It will obey whatever you tell it, right? If you tell Hey, find me 10 cluster k equal to 10, it will give you 10 cluster. If you ask it to give it 20, it will give you 20 cluster. So it will blindly obey you. But whether that 10 or 20 clustering makes any sense or not that is depend on your good judgment, your business decision, right? And that's why AI is not just science, right? Because application of AI is the technology and business together. So

Speaker 6  28:31  
so on that question that Ingrid asked with the different say there's a purple circle and a yellow circle, it's going to go ahead and put those in the cluster of circles and of circles, but then ultimately you're going to have to decide what that is, oh, this. We're gonna have to it's closing, or we're gonna have to set up another cluster, correct?

Speaker 1  28:52  
Okay? Because, because you are not leveling the data beforehand, after the computer does the clustering for you, then you have to look into the cluster and look at some of the sample data within the cluster, and then decide, okay, which group I'm looking at. Okay, yeah, okay, so going back to the process steps. So what if the program does, it will assign the data points to one of the K cluster depending on their distance from the center of the cluster, and then it will randomly assign the cluster to the centroid in the space. And then it will assign each data point to one of the cluster center, depending on the Euclidean distance, of course. And then it will recalculate that, and it will run it iteratively until it cannot find any further improvement. And that is where the good cluster is. So let's see whether these next animation runs. Sometimes it runs, sometimes it does not, oops, oops, this is the next animation. I don't know why this is not running. Are you seeing the animation running or not?

Unknown Speaker  29:55  
No, right? No.

Unknown Speaker  29:58  
Let me do one. Thing. Hang on a second.

Unknown Speaker  30:03  
May I ask the question as well? So for

Speaker 1  30:05  
instance, yes, okay, there it goes. Do

Unknown Speaker  30:11  
you see the animation running right now?

Speaker 1  30:14  
Yes, yeah. So see what it does. Is it randomly creates. So in this case, this example, we have given the program a k value of 1234, and these are the data points in your Euclidean space, right? So what the program just did, so let me see if I can run it again. How do I rerun? Let me refresh the page. Maybe that will make it run. So what the program will do, it will first put four data for center randomly, 1234, these green ones, and then it will start calculating the distance. And then based on the distance, it will then see, Okay, where can I move this center point so that the sum of all distances becomes minimized, and then it will run the same algorithm again, and then it will shift the center point again. So once it does it enough number of times, then at one point, the program will realize that well after this, even if I do more iteration, there is no further minimization of the total distance, and therefore it will stop there. So essentially, after running this algorithm, you will see that the computer has decided, well, this is my one cluster center. This is another, and this is another, and this is another, and these are the points that belongs to this cluster center, which here is depicted by these red lines connected, right? So sometimes you might say, well, is the is it correct? So in this simplistic graphical example, we are kind of showing the data points that are very cleanly separated in space. In real life, it might be little messy, and some of the decision might be little, you know, questionable, but hey, any machine learning is a statistical model. Right? Whenever you are running in a statistical model, it's a probabilistic is nature, and it is not deterministic. So yes, you might get things that that may not like each of the data points might not make sense, but overall, on a statistical level, of things averages out, and overall, it works, right? For example, in the middle data point, you might say, Hey, why is this data point? It belongs to this cluster. Maybe, if I move this cluster center little bit here, maybe one can argue this data point will be this cluster. But there is no point arguing that right overall, in the bigger scheme of things, things will work out right. And in order to measure the efficiency of this program, which you will learn later. That's where there are different measures of accuracy and precision that you can do. And based on that, you will tweak the algorithm to see which version of the algorithm provides you with a better measure of accuracy, better precision results, right? And that is, in fact, the goal of this whole boot campaign, right? For you to tune this algorithm to help computer come up with the best possible answers that it can.

Speaker 3  33:07  
My question was that kind of about the same thing. For example, I've created everything. I've got the result, but I need a different result. I want to see the different output. Can I run again? Or should I modify algorithm again to find the new

Speaker 1  33:27  
result Correct? What should is the process? Or can

Speaker 3  33:31  
I, can I switch the algorithm from, for example, from the bottom to the top? Or,

Speaker 1  33:40  
yeah. So there are many things you can do so, and that is exactly the process of developing a machine learning algorithm. So even if here for the simplicity, I'm saying, Hey, you use this algorithm for clustering. In reality, there are many, many different algorithm, right? So ideally, what you will do when you are trying to get the best result possible, first you will do is you will try different algorithms on the same problem, right? And then you basically pick some of the best candidate that seems to give you the best result. And then you are going to tweak some of the parameters of the application right of the algorithm, right? In this case, you will tweak the parameter k, right. Or you will basically tweak what your initial seed data points would be, meaning the location. So there are a lot of these parameters that you can tweak. And these are, in general, call hyper parameters, right? So, and these specific processes called hyper parameter to tuning, or hyper hyper parameter optimization, HPO, that is the industry name for that, right? So first is your algorithm selection. Next is your hyper parameter optimization. And all of these takes lot of iteration, lot of trial and error, and finally, hopefully will come arrive at a result that kind of satisfied your requirement, your needs, right? Your business needs. Hi, one quick

Speaker 2  34:59  
question, actually. And so whenever the initial grouping or whenever the initial grouping comes, so would we say that this came in clustering is the one to start with, sort of machine learning. Or can we put it in that way?

Speaker 1  35:16  
Yeah, I'd say, as I said, Hold on to that until we come to the actual payments clustering class, or any clustering algorithm class. This is one of the simplest algorithm to understand. And as I said, in each of these problem domain, there are many, many different algorithm right? The focus of today's class is not going to explore all of the class, or, sorry, all of those algorithm. The focus of today's class is to just to give you a conceptual level idea, a theoretical understanding of what are the four broad class of machine learning on. Okay, so let's move on. I'm trying to going to try to put it back in slide show mode again.

Unknown Speaker  35:57  
Oh, mount is showing me. I

Speaker 1  36:06  
Okay, okay, so moving on. So now we are going to talk about that other algorithm, which basically going back to this time this slide deck here the dimensionality reduction, which I know that it kind of sounds gibberish. It does not sound English to many of you. What the heck is dimensionality reduction? So we covered the clustering and one algorithm. Now we are going to understand what dimensionality reduction mean and understand one algorithm for doing that. Okay? So first, let me try to help you understand what that means, right? So, in the previous problem, we talked about these n dimensional hyperspace, right? Which sounded very cool, sci fi, right? Well, guess what? There's a problem. What's the problem. The problem is an algorithm as simple as your k means clustering, which is basically find the data, find the distance, and move the cluster center, minimize the distance and keep doing it over and over again. Even a simple algorithm like this can be costly, can be very computationally intensive and demanding. If your n is too high, N, meaning your if your dimension is too high, like if you have a dimension, if you have a space with 10 million dimension, then your program can become very slow, right? And then sometimes lot of information is may not be good also, because it can also create more noise. So when you have lot of dimension, a little variation of your data point will make it seem like amplified, right? So then any error, any noise in your data, which is kind of a matter of fact, like it's a reality, right? So whenever you do any data collection, there will be some error in your data collection, and that's what we call the error noise. So when you use the data in its in a original dimensional space, not only it becomes computationally extends expensive with but it also amplifies any error that you might have in the data. So sometimes you will see it will be better if you take the data in a higher dimension and compress it into a lower dimension, right? So from a 1 million dimension, if you can compress it back down to, let's say, 1000 dimension, that's a big win. That will hugely improve your performance of your algorithm, and you will be surprised to see it will actually improve the accuracy and precision score of your algorithm as well, even though, in going from a higher to lower dimension, you are probably losing some of the details. But that's okay. Losing details is not too bad. Like, think about a completely different example. Let's say you take a picture with your with your digital SLR camera, right? So you take a picture in the raw format, and you put the and then you take the SD card out. Look at the file size in the SD card. You will be astonished, like one single picture could be hundreds of gigabytes, right? Two, 300 gigabytes of one single picture. And then you open the picture, if you do not have a super high power machine, you will see, just to render one picture, it will like, slowly render from top to bottom. Right. You have seen that. Many of you have seen that. So which is good when you are trying to do some pixel like, a bit level edit to the picture, right? If you are into that kind of thing, you really want that. But for most purposes, we don't really want that, because we cannot work with such a large file. I mean, I mean, even rendering it on a screen is a pain, and forget about sharing the file or posting it somewhere. So what do we do? We convert it to a JPEG file. So. And suddenly that 200 gigabyte file comes down to 20 kilobytes.

Unknown Speaker  40:07  
How does it do that? Have you thought about it?

Speaker 1  40:13  
It's the same Algorithm, Dimensionality reduction. So each of the pixel right has lot of data. So in that algorithm, what you can do is you can take a group of pixel, let's say, instead of rendering one pixel each you let's say take a group of pixel, let's say group of 100 pixel, and average out whatever their RGB value is, and you show it as a one pixel. So that's how you reduce the dimension, and it so happens that you don't lose much for naked eye. You don't even feel a difference, right? Even though you are going from 200 gigab gigabyte to 200 kilobyte, it works just fine. So that's essentially what dimensionality reduction is, right?

Unknown Speaker  40:59  
So

Speaker 1  41:01  
in a small dimensional scale, you can think about this way. So let's say you have a xy plane, right? You have data all around, scattered over the xy plane. What if, if I only look the data from the top and only look at the x values of the data, what would you see? So each data let's say, think about this. Let's say you have a data plane like this, and you have all these data point in the space. What will happen if you shine light from the top, you will have a shadow of each point on one single plane. Now, if you take those shadows, so suddenly you have a dimensionality reduction, because when you have a 3d space, each of the data point hanging in the space have X, Y and Z coordinate. When you are shining a light and looking at what are the shadows these points are casting, since the shadow is on a single plane, now you have only X and Y coordinate. You kind of foregoing the z coordinate for the data right? So that, in effect, is a very good conceptual way of understanding what dimensionality reduction does, right? So the algorithm that I wanted to talk to you about that does that is called principal component analysis, in short, PCA, which is the most widely used algorithm for dimensionality reduction. What this algorithm does is it maps data in a higher dimensional space to a lower dimensional space, but preserve most important relationship between the variable without any prior knowledge about the target. And it is used to simplify complex data set, making them easier, easier to understand and work with. So that's what that does. And this time, I'm not going to even go and read the process steps. You will have the copy of the slide. Feel free to look into it if you want to get ahead. A few weeks ahead in the boot camp, you can always google search and understand, try to learn the algorithm itself, right? I just want you to put the process tips out there. But we are not going to discuss the algorithm in this class. I'll just try to show this as an example. Yep, these application actually works. So this animation actually works. So in this animation, what I'm trying to show here is think about these data points, all these blue dots in this two dimensional space, right? So here my source data set is in two dimension. If I want to do a dimensionality reduction, there is only one other way that I can go, which is from 2d to 1d now how do I go from two dimension to one dimension? Well, one thing I can do, I can draw a straight line through the data in any orientation and then drop vertical line from the data points to the actual straight lines, kind of like shadow, right? So think about you basically put a straight line in any orientation and then shine a light from perpendicularly right, and then it will basically create the shadow, which is basically these lines are right. And then all the shadow on this single line, they will basically show the data in one dimension, because single line mean one dimension right. Now the question is, which way you draw the line that makes the best estimate of that of the data in one dimension without losing too much of it. And that is what this algorithm does. The algorithm essentially it tries to draw this line in various orientation, and it keeps iterating. It tries different thing until it finds the line that reduces the loss or minimizes the loss that captures most of the uniqueness of the data. So in this case, the animation is showing is that algorithm is trying different dimension. But you can probably see from intuition. And like right at this point, like when you see this little two purple thing, you will see that when this black line is aligned to these, that is when the loss will be minimized, right? So if you run these data points through the PCA algorithm, you will basically get a single dimensional data points right aligned to this from here to here. That is essentially what the algorithm will do. Now, when you are actually working with real data set, you will never see this, because this is only possible if you have only three or three dimensional, two dimensional data. But the real problem that you will work on will always have a higher dimension, because you are actually working with real time, real life data. You are not working with just points on space, right? So because it's not a science project, it's a real world data. So you will never see this in a real world, but it's always good to look at this kind of animation and have these discussions, so that in your mind, you understand what the algorithm is actually doing under the belly, under the hood. So that's that, that is your dimensionality reduction question. And if you have any question about the details of the algorithm, how the algorithm works, I'd say, hold off on to those, because that's that's not the purpose of today's class.

Unknown Speaker  46:24  
Daniel, go ahead. So

Speaker 4  46:28  
minimizing the amount of them a loss of information based. It's not a based on the x, x, x axis or the y axis, but it's a compromise between both accesses so you can minimize the loss of data. Is that correct? That is correct. Okay, thank you so much.

Speaker 1  46:50  
Okay, and then lastly, some of the application of unsupervised learning. The first one is exploratory data analysis, which is kind of what I talked about when we share that example with you, when, let's say, you have this customer data, and you are trying to understand how many different distinct demographic groups you have, right? So those kind of analysis is called the exploratory data analysis. You have a bunch of data, and you want to explore how many different kind of thing that you have in your data, and how to how to slice and dice your data in the best way possible. So that is one application, consumer segmentation, which is a specific form of that, which is basically what I'm referring to, recommendation system meaning. So let's say when you are on streaming site right, and go to netflix, go to YouTube right based on the content that you are seeing, you will see that it will suggest more and more content, which will recommend more content. And that essentially is unsupervised learning, because no one tells the algorithm behind YouTube like Mr. XYZ likes to see the movie where Tom Hanks is present, where the movie is shot, between 1970 to 1980 No one says that. All it does is it basically looks at your movie playing list or movie or video anything. And it basically it has some attributes of those movies or videos in its database, right? And we don't know how many, how the what those attributes are, and these are the attributes that creates an n dimensional data set, right? And some of the attribute could be the genre of the movie, which year the movie was shot, who are the key, what is called the key actors in the movie, and so on, right? The So, based on all of these, it basically finds the pattern, the cluster of the different classes of movies or videos, and then, based on what you watch, it will then try to put you in one of these classes. Like these is the guy who kind of likes these, category one movies more than category two movies, therefore I'm going to recommend more of category one movies to this person. So that's what I mean by recommender system or recommendation system, right? Another example of that is anomaly detection, detections, right? So anomaly detection, meaning any crosses that is supposed to happen in one way, suddenly, there is a defect, there is a bug. Now, obviously, when that happens as an engineer, let's say if it's a production line, right? Let's say you go into a GM factory where the cars are coming out and a production line machines are working on that, and everything has to work like clockwork, right? If something goes wrong, then we have problem. But if something does go wrong, so what happens in this day and age? There is a term in in the manufacturing industry called digital twin, right? So everything that is happening physically in the machine has a digital footprint that it generates all the time, like every millisecond it creates a digital footprint. So essentially, in the digital world. All the machine have a twin, which we call digital twin. So if the machine does something wrong, if machine faces some unexpected obstacle, if machine is trying to apply, let's say, coating, and suddenly a tiny speck of dust came in, and the pressure sensor suddenly measures a little difference of pressure between the two surface that are pressing against each other. So out of the million sensors that are embedded in that surface, maybe one of the sensors will detect a slight variation of pressure. How do you figure that out quickly? I mean, it seems like a hopeless situation in the surface, right? But that's where you do this kind of unsupervised learning, where you know your algorithm knows what is the pattern to expect. And the moment there is a deviation in that n dimensional Euclidean hyperspace, suddenly you will have a point that is kind of little bit far away from the cluster center. And then your machine will raise an alert in the digital world like, oh, that something goes wrong. Maybe you should go and check that out, right? So that's what the analytic detection is, right?

Unknown Speaker  51:05  
Yes. Jesse question,

Speaker 7  51:07  
So in your example, with like Netflix, would a thumbs up or a thumbs down be an example of adding supervised learning in that case?

Speaker 1  51:16  
Yeah. So see all of these companies, they use machine learning in various shapes and form, right? So when we say recommended system, it's, don't take that as a guarantee, or my word that Netflix only uses that particular algorithm, right? They use a lot. So obviously, in the Netflix, or even in a Facebook, when you like, they collect all of these data, right? And in fact, that that is something that you can make the topic of your essay, that you are going to write in this big challenge. Like, hey, go dig some research. Do some research, and dig in and see, like, take one small problem. If you try to think of Netflix as a whole, you'll kind of be, probably be lost. Think about in one problem narrow context, and think about what way that company could possibly use machine learning. And often time, you will see, when you do the research, do your own reading, you will see it's usually a combination of many, many different algorithms and techniques that companies use to deliver a certain feature, product feature to you as a consumer. So

Speaker 1  52:25  
cool. So ready to move on the next class of algorithm, which are supervised algorithm, right? So this is now we are talking about two different types of supervised algorithm, and they are who remembers the two different classes of supervised algorithm that we showed? Couple of slides back, anybody? Classification and regression, classification and regression. Now we are going to talk about those. So supervised learning in general is when you still have multiple classes of object. But this time, you are not presenting the data point as objects to your program. You are actually leveling like when you are providing these you are saying, hey, this thing is an apple, this thing is an orange, this thing is in banana, and this thing isn't is a pairs, so you basically provide give some sample data that you as a human have looked into and has already identified, and that is the supervision, and then, based on that, then the machine will basically be able to identify, Okay, So there are 100 different object. There are 25 apples, and these apples are somehow similar, so therefore the apple group must share some pattern, and the banana group must share some another pattern, and so on. And based on that now in future, when a new picture comes in, and if you ask the program, hey, Mr. Computer, tell me which of these fruits you are seeing out of these four then the program will be able to tell you, yep, I see an apple or I see a banana, because it has already seen many, many pictures of apple and banana before, which you told the program, by the way, these are apples and these are bananas, because this is supervised learning, right? And this example that we talked about just now is called classification, which is different from clustering, by the way. I mean, on the surface, it might kind of sound the same, but clustering where you have no knowledge, no prior knowledge about what you are looking at classification meaning where you have prior knowledge. So k is not arbitrary. Here. Here your k is known. You know how many different case you have, and you know what the names of those k's are in those groups are okay. So two kind one is linear regression, and one is called we'll both. We'll talk about the other one. So let's talk about the linear regression first. First. So the this kind of data, what it does is it computes a linear relationship between the variables, the dependent variable, and one or more independent features. So this is again, going back to if some of you probably have taken, I don't know which high school math class would be, probably pre Calc, where you have a whole bunch of data point and you have to come up with a line that best connects all the dots, right? You have, you remember, you have probably have done that kind of problem in high school pre Calc. So that's essentially what it does, except, again, in an n dimensional hyperspace, right? So if you have like pictures, let's say 100 by 100 pixel picture of things, then essentially you have a 10,000 dimensional data space with different pixel values, right? And then all you are trying to do is like, Hey, I know that if pixel values are so and so. Then this combination of pixel values give me this data point. So therefore, and I have multiple of these data points in these 100,000 dimension space. How can I draw a straight line that basically connects to all of these so that in future, when a new data point come in, all we have to see is, okay, this is a new data point where in the line this data point belong, and that would be my predicted value, the outcome of that data right? So that's essentially what it does. So it treat dimensions of data points as independent variables, and the training level that you are providing that it treats as a dependent variable, like y and x, right? So X is basically independent variable, which is like the coordinates of your data points, and y is the level that you are providing, right? So it's basically a function y equal to f of x, right, like talking in pre calc one, right? So y equal to f of x, where we don't know what this f is. All we know is it is some function f, but we don't know what that is, and that's exactly what this linear regression does with the assumption that f is not some weird function, like it is not some weird curve or like Sigmoid curve or parabola, nothing like that. It's simply a straight line or a plane, right? That's why it's called linear right? So the if we, if we think of this way, then, then the solution basically becomes very easy. All you have to do is you basically derive a linear hypothesis function and compute the what the error is by calculating the distance of this line from each of the data point, and then tilt the line slightly and see whether the errors are increasing or decreasing. If, say, let's say you have a whole bunch of data point, and you first draw a line like this, and then from that data point, you connect all the other from that line, you connect all the other data points, and see what is the total root mean square distances. And then you tilt the line one way or the other. And if tilting one way, you see that total error goes higher. You don't go that way. You till the other way, because your goal is to minimize the error. And then you tilt a little, you do the measurement again, and tilt a little, you do a measurement again until it comes a point that the error is the minimum possible that you can get right. So essentially, it's a minimization problem, and in this it basically kind of showing that. So here you see these blue dots are kind of scattered. Where the algorithm is trying to do is trying to find a line that best describes the behavior, the linear behavior of the line. So initially it will start with a line with an arbitrary orientation. And what it is doing is it's basically in each iteration, it is trying to find the distance, total distance of all the data points from the line, summing them up, and seeing whether that is lower or greater than from the previous orientation of the line. And it keeps doing that until it can find a minute so that essentially is your linear regression so wherever it stops. So at some point it will stop right where it cannot improve anymore. And that point, the line that you will have, that is your regression line. So what that means is, when you have that regression line, the line will have a format like y equal to, let's say, 5x plus six, something like that, right? Like a, 1x plus a, 2b A, one A X plus B, general format. So when you stop there, then any new x value comes in, all you have to do. Your prediction becomes very simple. Then you already have the linear equation, which is the function. And then you plug in a new x value that you have not seen before at that point, it's a simple algebra to give you the y value, meaning the output. So that's essentially what the linear regression algorithm does. This is probably the easiest to conceptually understand out of all these four to me, at least. Okay. Okay? And then the last one is your classification problem under supervised category, right? We talked about clustering under unsupervised, but this is classification under supervised. And most commonly used algorithm, or at least the beginning algorithm, I should say, people start with, is called a nearest neighbor. You see the word k here, I mean the symbol k here, the idea is kind of the same here. Also, you're trying to find different cluster sets, right? So, but here, unlike your k means clustering problem. Here, you know, what are the different clusters you have already and then what it will do is it will select the number of nearest neighbors for prediction and measure the similarity by computing the distance between the target and each data point. So essentially, how it works is so let's say you have these two different data points. So here we are showing in this animation and red and blue two different categories so they have different data point. So what the program will do, it will create what is called a decision boundary between the two, essentially right. And the decision boundary will be kind of a curve in a space that will cut through the middle of the points in as neatly as possible, so that anything on one side of the car will belong to the blue class, because the blue class will be closer to them, but anything on the other side of the car will belong to the red class because that will be closer to them, right?

Speaker 1  1:01:34  
So think about, let's say, two different enemy groups soldiers, so they have their this position, right in a battlefield, right? So now, if you have to find like, Okay, I want to kind of be, let's say I belong to the blue soldier, right? And depending on all these soldier position, I know how much they can fire. So I want to be set myself up in a way, so that I am as far away from the Reds groups as possible, because the red groups are enemy group and blue are my friendly group, right? So what I will do, I will basically measure, okay, how far these guys can shoot from their respective position in the battleground. And then I'll draw a, somehow, somewhat a boundary, right, which is a class boundary. And then if I want to stay alive, I better stay on the blue side of the boundary, right. So that's essentially what the nearest neighbor calculation does,

Speaker 1  1:02:34  
except in this case, if you have pictures of bananas and apples and oranges instead of having red and blue dots. So those pictures will not be represented on a two dimensional space, but depending on the features of those pictures and the pixel values and so on, each of those pictures will be represented in a multi dimensional, higher dimensional plane, right, depending on how high resolutions the pictures are. But the concept is essentially the same as you're seeing in a 2d Okay? And one thing I'll tell you, the beauty of machine learning is that all of these internals of these algorithm you really don't have to do the math, because all you are going to do is you have to know, okay, which does which? And then you have to know what is the Python library that I'm going to use for which. And then your job is simply to do the run the experimentation multiple time with different parameters, slightly different twinning of tuning of the input to the program to make sure which combination gives you the most accurate value. But you don't ever actually have to write the algorithm yourself, because that will be too much. And those, that's hard problem, and that's already solved, right? So we don't want to get there.

Speaker 1  1:03:53  
Okay? And then lastly, some of the application for supervised learning. These are kind of self explanatory at this point, I suppose, image classification, speech recognition, weather forecasting, fraud or spam detection. So I hope you can see why all of these belongs to one of the classes of supervised learning, right? Because in all of the class, all of these cases, you have previous sample that we you as a human, know which is which. And then you feed all of these data, along with your training, meaning your classification, to the computer, and it basically trains this regression and classification model to basically achieve these things.

Unknown Speaker  1:04:41  
Okay, cool question.

Speaker 1  1:04:51  
Okay, I will take that as a no moving on, and this time, I'm just going to go very quick through the neural networks. Because this is kind of premature to talk to a lot about the neural networks, but just know that neural networks are basically a kind of technique that you can use to do any of the previous classes of problems like talk about regression, classification, clustering, dimensionality reduction. You can do all of these problems using neural networks. So it is basically a parallel technique of solving essentially the same problem, which you could do in all of these previous four algorithm. But there we are using mathematical and statistical modeling in neural network. It uses an alternate approach of solving the same problem, and that approach is basically something that is kind of loosely modeled after how the human brain works. So in human brain, we have these things, cells called nerve, right? So you kind of know, like the basic biology, probably, I was very bad at biology in high school, but even I know, like, there are probably not sensor going from our eyes and tongues and nose and ears and skin all over the place, right? It is somewhat somehow connected to our brain, and then we can basically sense things based on these sensors, right, and from the very moment that we are born, or maybe even before we were born, right, even when we are like inside, inside our mother's womb, we probably start capturing data through these inputs, right? And hundreds of millions of them, right? And over time, these networks kind of learn which is which, and that's how we make sense of the world around us, right? So this class of algorithm are kind of loosely modeled after that, that philosophy, except, instead of actual biological neuron, thing connecting, there is no physical neuron, the whole thing is modeled mathematically inside the memory of a computer. So idea here is you have a whole bunch of trend like, whenever you have a training data given there would be a there would be a sensor or a input right, which, in this case, we also call it neuron, much like its biological counterpart. It's called a neuron. So the neural will basically decode the data. So the data could be any complex thing, it will decode that as a point in your n dimensional hyperspace. Then it will feed the data with bunch of other neurons, and those neurons will calculate some kind of a mathematical function, and all of those function will be feeding to the next layer of neurons, and so on and so forth. And finally, the final neuron will be able to learn when you train it enough number of times, just like a baby's learn a newborn learn when you when it sees a one thing, enough number of times, then the baby knows. Okay, this is my mom. This is my dad. This is food. I have to eat this, right? So, if the concept is the sale, right, so the advantage of this is it can actually detect very complex relationship within data in the case where the statistical modeling that we talked about using four of the different classes of algorithm. Sometimes they fail when your model, when your problem is very complex, they do fail miserably. And magically, the neural network actually excels in those classes of problem, right? So that's the that's the advantage of that, right? So that means it can, you can actually work with lot of messy data, with lot of noise, lot of error in the data, and neural network will still be able to figure out the underlying pattern from the data. But the disadvantage some people do not like is, hey, it's kind of a black box, like, unlike the statistical modeling, you can actually know how it is going. Well, maybe you and I don't know, but there are smart people in the world, enough of them, who knows what is going on. But guess what? This neural network, the large neural network, it kind of eludes the expert themselves, like the creator of the neural network doesn't even know what exactly is going on under the under the hood of a neural network, right? So a lot of people, that's why do not like and that's why you will probably see sometimes there is outcry against AI, like, hey, our fate is being determined by a black box, by a machine, which we don't know what the machine is doing and why it is doing. And some of those concerns are, are solid, valid concerns. But that's why, at the same time, there is other companies are slowly realizing the kind of a responsible AI is something that it's kind of a part of it being a good corporate citizen. So they are, just like your financial controller, your ombudsman, they are having this kind of a control for to control the quality of the prediction, to make sure that the model, the in this black box model, do not tend to develop some unwanted bias among the data set that it is looking at and so on, right? And it's like you can even make these as a subject of your essay in this week essay, right? Like talk about responsible AI, right? And do some reading and see how companies are kind of trying to be better at that, right? Daniel.

Unknown Speaker  1:10:00  
Explain, prone to overfitting, please.

Speaker 1  1:10:04  
Oh, prone to overfitting? Yes. So growing to overfitting basically means sometimes, I said, right, sometimes less is good. So what happens is neural network like these things, if you have lot of these neurons, right, and if your data set is not too complex, then it will basically tend to think like so I'll tell you in a plain English, right? So let's say I want to train a baby with what is an animal, right? And all I keep showing the baby is different breeds of dogs, right? Let's say sometimes I show him a German Shepherd, sometimes multi sometimes a poodle, and so on. But whatever I do, I always show him some kind of a dog. So if the baby has never seen any other animal in his life other than a dog, then tomorrow he goes out in the world and he comes across a horse, and it will think it's a dog, just a big dog, right? Because the poor guy have never seen a dog. All it is seen. So sorry, not seen a horse. All it has seen is a dog. And his mob told me, even though all of those animals look very different, but he's told every time, hey, this is a dog. And now the way he goes out in the world and sees a horse which also has four legs and kind of walks the same way it must be a big dog that essentially is over fitting, right? So when you have data, but you do not have enough variation in the data, and then you put the data through a very complicated neural network, it kind of tend to overfit on certain classes of data. So let's leave it there. When we go to that part of boot camp, we will talk more about it. Okay, cool. And then deep learning, which is basically nothing by neural network, which is more than one layer deep, right? So essentially, technically, you if you have more than one layer between your input layer and output layer union neural network that is deep learning. So even if you have two layers, that is still called as deep learning, but know that in today's state of the art neural network models. It's not 235, or 10 layers. There are hundreds of layers, and every time you add a new layer, the ability of your algorithm to detect the pattern grows exponentially. But at the same time, the computing need also grows exponentially, and this is what remember yesterday. I was talking about AI winter between, like towards the end of 1970s to mid of 1980s where kind of interest in the field started to die down. And that was the time where, just before that, we found that, yes, artificial neural network is something that we can produce great result, but in reality, the theoretical model was not being able to perform when faced reality, because we did not have enough computing power, right? Even two or three layer neural network would bring the whole machine down back in this those days, and now we are talking about 234, 500 layers of neurons, right? So it's a totally different world. That's why, and that's why I'd say sometimes you will hear concern like, oh, AI is going to ultimately take over the world. If you think about it this way, you might see that, well, there is some ground. Because think about it, how we become so intelligent? We are independent thinkers. We are creative, right? We have ability for judgment, but no one is telling us this. Like, think about forget about creativity. Think about ethics and morality, right, even if you are not told about any ethical, moral behavior, Code of Conduct as a person, as when you are growing up, just from your the way that you are dealing with your environment, you kind of know what is ethical, what is unethical, right? And that's a very uniquely human trait. But how do we learn that, even if we are not told? Well, it turns out that we have so many millions of neurons that it kind of automatically figures out what is, what is belong to ethical group and what is belong to unethical group. So thinking of that way, it is kind of a classification problem, right? So ethical versus unethical in a very, very complex world. So if we could do that with millions of layers of network, right? And computers today, if they can do two to 300 network, then who knows 1020, years down the line, computers would not be able to do million layers of network. If that day ever comes, then I would agree that artificial intelligence probably match or surpass the human intelligence. But we are not there yet, even in these. Day of supercomputing. We are still far, far away from actually mimicking the scale of human brain, the biological brain, using the mathematical model. But we can do lot today, then we could do 2030, years back. And that's why all these gbts are coming up, right? Because these are nothing but 300 layer deep needle networks.

Unknown Speaker  1:15:23  
Okay, so I will leave you there.

Speaker 1  1:15:27  
Let's move on. Yeah, this is kind of self explanatory. I'm not going to go into that. Natural Language Processing is basically just one application, sentiment analysis, understanding the spoken words. These are just some of the applications of neural network, large language model, right? Talking about the GPT, GPT is nothing but one of the class of llms, or large language model, which I'm sure you have all heard about, right? I mean, it's amazing how these GPT LLM last three four years, within last two to three years. I think these words kind of became household names before that no one even know, like, what is LLM, English, please. Well, now we all know what LLM is, right? Large language models, which is basically nothing but a couple of 100 layer neural network which is trained on lot of English language and text data and so on, so they can very easily understand the pattern in the in the human like text, right? So that's all that is cool. Okay, I want to do a quick demo. Have you any of you seen the teachable machine website? This is like a nice, cool toy that you can play with? Okay, so let's try to do this, I have to first, then stop this

Unknown Speaker  1:16:47  
and then go to

Speaker 1  1:16:51  
teachable machine. Let's go with the original one v1 Do you see my screen? Okay. Hey, someone from our TA team, can you slack this link, the teachable machine link?

Unknown Speaker  1:17:12  
Or you can just type teachable machine. I'm sure it will come up.

Unknown Speaker  1:17:18  
Okay, so essentially, it kind of shows again,

Speaker 2  1:17:23  
I'm sorry, I was just saying, I'll get but okay,

Speaker 1  1:17:28  
so what it does is it basically shows a very cool demonstration of what we just talked about. So it is basically under the hood. It is a deep learning model that is being trained for a classification problem on video feed, and that classific video feed will come from my webcam here, right?

Unknown Speaker  1:17:50  
So you can still see my screen, right?

Speaker 1  1:17:55  
So what we'll do is so there are three different classes that I'm going to train this model on and depending on each class. So if it is the first class, the output will be like a cat picture versus a dog picture. And what is that? Is that a squid, rabbit

Speaker 2  1:18:15  
or something, some cute little roses, something I just put, I put the link on the 01, live channel on Slack. Yeah. Okay,

Speaker 1  1:18:23  
cool. Okay, so now let's train this model. So I'm going to train the first model to identify that lets you explore how machine learning works. Oops, you can teach the machine Yeah, so mute your thing, or just, just watch this right now and then you can play. So what I'm going to do is I'm going to teach the first class to be a man with his hands raised, okay, so what I'm going to do is I'm going to put my hands up, and then I'm going to hold this button for some time, and you see that it's showing that it is generating many, many different examples of things, right? Okay, so I have like, 175 examples. So now what will happen is, if anytime it sees my hands, it up, hand is up, it will show there is a cat video. Currently, my hand is not up, but it's still showing the cat video. And this is an example of overfitting, by the way, going back to your question, Daniel, because it doesn't know what else all. It knows that there is a man. Even if I'm not putting my hand up, it still thinks it is the green class. Now I'm going to add a second class, which is, let's see what I can do. Maybe put a hand up my head like this. Okay, so let's do that. I'm going to move little bit so that it can kind of get from different perspective. Yeah, I have had enough, so now let's see what happens. Now I'm going to put my hand over here you see how output change. So now you know a man with his hand above his head is sparkle. And if I put like this, you see, just by moving my hand, I can change what the output from the neural network is, and then I can train a third class which, but you got the idea, right? So essentially, this is a real example of actual deep learning neural network that is looking into the video feed and trying to figure out which of these different poses that you are holding right now from your webcam bit. Okay, very cool. Play with it. Don't play with it too much. Just play with it little bit. And then you later when you will have more fun when you actually code this yourself, right, using Python. That's the most fun part of the bootcamp, by the way. Anyway, cool. I think we are done with this part, and then we will go into the GitHub demo part, which is the second part of the class today. Any question before we do that?

Speaker 1  1:21:07  
No, cool and today, also, specifically, I did not give you guys a break because we were having, I think, a fun time, but I'm going to give you that break now. I think you all deserve it. So now it's 1053, my time. Let's come five after right? So 805, Pacific time. That sounds good. Okay, and then we will start fresh with the GitHub, hands on demo. Okay?

Speaker 1  1:21:43  
That could be Word Excel, PowerPoint. So what are the different things that you have used in your work or school?

Speaker 1  1:21:59  
Google Drive. Google Drive, yes, I was expecting that answer. What else Dropbox? Dropbox, yep, I expected that too. There is another big one missing, yeah, of course, yeah. I mean, if you come from a technical background where you have actually done coding, GitHub, what else more like in an office environment? SharePoint, Microsoft. SharePoint. Yes, that's what I was going to say, SharePoint or OneDrive, for that matter, which are kind of connected these days. So I love Microsoft ecosystem, though, how everything kind of comes together with your teams and with your OneDrive on your SharePoint. So there are many different ways that you can share files, right? And one common thing about in among all of these technologies is basically, it gives you a way to have a copy of your files somewhere up there in the cloud, so you don't lose your code if sometimes, let's say, by mistake, you delete your file from your local computer, there is a way you can get back to the file from the cloud copy, right? That's one benefit. The other benefit is, let's say, if you are making changes, changes, changes, and sometimes they say, Oh, your manager says, Hey, I like that thing that you did in your slide last week, and you deleted the slide, right? You can easily go back in the history and recover that slide, right? So that's another thing. The other thing is, if you are working, let's say from sitting somewhere, let's say in Salt Lake City, and your co worker is in New York, and you guys are working together, you are collaborating. So you make some changes, your coworker makes some changes, and it all goes into one place, and the work comes together, right? So shared collaboration, right? So these are some of the things that you are already aware of, especially after COVID, where we all became remote worker. We are using more and more these technologies, right? So GitHub is very similar to that. It allows you to store your code, collaborate with your peers on the projects, and in this case, submit the assignments, because you would actually have to create a GitHub repository for your assignments to be submitted to the instructional team, right? So that's why you need to use GitHub. So what we will do is we will do some demo on how to use GitHub. And there are actually many different ways of using GitHub, and I'm going to show you a couple of them, but depending on what kind of computer you are using, what operating system you are using there are many different ways that you can use GitHub. Okay, good. I have one question, when I like what is what difference between this GitLab GitHub, because I keep on hearing a lot of jargons around there. Is

Speaker 2  1:24:57  
there any major difference between Git and GitLab or. GitHub

Speaker 1  1:25:02  
for your purposes, it's all the same. So GitLab is basically GitHub is, was the, basically the open source thing, and GitLab is basically Microsoft's own version of that. Thank you. But all of the command that works

Unknown Speaker  1:25:19  
for Microsoft

Unknown Speaker  1:25:22  
owns GitHub.

Unknown Speaker  1:25:24  
Get right, but

Speaker 2  1:25:26  
Microsoft acquired it, GitLab. I think it's a different company, but GitLab,

Speaker 1  1:25:30  
is it more towards Is it the maybe I'm wrong. I thought it is the other way. No.

Speaker 2  1:25:36  
GitHub recently, relatively recently, bought Microsoft.

Unknown Speaker  1:25:40  
Okay, okay, yeah,

Unknown Speaker  1:25:45  
yeah, but all of Gill

Speaker 2  1:25:47  
lab is more commonly for like, enterprising companies, and GitHub is often more individuals, more for

Unknown Speaker  1:25:53  
individuals, teams and stuff, yeah,

Speaker 1  1:25:59  
okay, but the concept is still the same, right? All the commands and everything, it basically works across. So both run the software get. So yes, software is get, yeah. So what get is, essentially, it's a way to keep track of your work over time, right? So let's say, at any given point of time, let's say you start a project, you create three files right as part of your work. So file a, File B and file C, you upload them into get, or in this case, commit them into get, and get has a version of this. And then over time, as the time passes by, you delete some of these. You change some of these like, let's say File A now becomes file a one a new version of the file, right? Whatever change you do. So these versions are basically the commits that you are doing. So let's say initially you have three files. After a day or two, you change something on the first file and you deleted the second file and you do a comment. So in the second version, you will have the new version of the file, and deleted file will be omitted. And then you also have a new version of the start file, so that would be your version. In the next version, you probably have decided to delete the first file for some reason, and you have a another version of this file, so you have the second version and so on. So the idea is that no matter what you do, it keeps all these version history. So at any point, even though, let's say you are in version five, you really have not lost anything, you can easily go back in time, and you can get your file from any previous commit that you have met. So essentially, it acts almost like a time machine for your files. Okay? So one concept of in Git is a branch. So a branch is basically so in this picture here. So what we are showing here is this timeline that check ins over time you are basically doing this checking across a certain Branch. A branch is basically another construct that allows you to group work done by different people. Why that is necessary. So let's say five of you. So let's say we put you in a team and you work on a project right, which will be in the week seven and week eight of this boot camp. So five of you work on a project, and your project is in a good shape. After working on it for two to three days, you create a main version of the project, and you save all the files, and then there is still few days left. And then you guys are still working to improve your code in some way, but you don't want to touch the main version, the main branch of your program, because that's a good way you know that it works. Yes, there is room for improvement, but you do want to touch the main version. So essentially, what you will do, you will create a separate branch, and any further work that you do, you will do it on a separate branch without touching the good one, the good branch, which you call the main branch, and any other work that you do, you do it in a separate branch. Also, let's say you have some division of work within your group, right? So let's say your project involves creating the algorithm, which is being done by someone and then someone else is doing developing some web application, some like a graphical user interface around it. Someone else is doing some more data analysis part. So let's say three different groups of people are working. You don't want the work of one group to kind of mix up with the work done by other, right? So you can create three branches, one for machine learning work. You call it whatever. You call it ml, Branch. You can call another one, say data branch. So anyone doing any data engineering work will work there, and then anyone doing your web work, maybe you have a web branch. So that is another reason you will need to use this branching. Another use of branching is for develop, sorry, for testing purposes. So let's say your team is divided in a way that there is a group of people who develops the code, a group of people who does the quality control, like the testing and everything of the code, and you don't want the quality control people to touch the code that the developer is doing. And also you don't want to have the developer touch the code that you are finally going to ship out as a final product, right? So now, for that, also, you will need to have three different branches, so that one group of people does not step over onto other group of people's work, right? So you will have a main branch, you have a dev branch, and you have, let's say, a test branch, right? So these are just some of the example of why you need to use branching on GitHub, right? So the idea here is, if we break something while working on our code, this will allow you to restore the working code from a good branch that you have saved earlier, and do not touch that, right, like, like a good copy of your work. So that's what branching does.

