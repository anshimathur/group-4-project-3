Unknown Speaker  0:05  
Hello everybody.

Speaker 1  0:08  
How are you guys all feeling being in the last week of classes today and the next two week is all yours to show all of your cool stuff, the things that you have learned over the course of what 22 weeks now, wow. Yeah. Okay, cool. So this week,

Unknown Speaker  0:36  
we are basically going to

Speaker 1  0:41  
spend time to ask question to the LLM models. Okay, am I sharing my screen? Yes. Okay. So over, over the last few weeks, we basically learned about how we can build more and more substituted large language models, right? Using using our well known neural network architecture, right? You have seen how we have kind of increased our model in complexity. And finally, we were able to actually do a rudimentary text generation that model with proper embedding, we learned that if you can generate good embedding at a higher level of abstraction, not just at a word level or at a sentence level, but more at a topic level embedding. So then, if you basically train this model with large data sets, the models are able to generate embedding for different topics or sentiments or themes within your text, and that is behind the success of today's large language model, where you can almost ask these models anything using human language. You can ask simple question like, Hey, who is the current president of United States? And you will feel like, well, I cannot also get that answer from Google search, but these models go above and beyond than simple Google search, right? So these models then basically go and understand the intent that we are asking to the model and generate embedding like the question that we provide, it generates an embedding, and based on that, it finds between in the universe of all other embeddings that it has generated by crunching through millions and millions of web pages and books and basically almost everything that we have digitized as humanity so far. So everything has been vectorized and embedded in these models, so much so that no matter what question you ask today in these large language model to these large language models, they would be able to find, okay, what is the where in the universe of all these large embeddings that that basically models different sentiments, thoughts, teams and all where the does your question fit in? And based on that, it will basically do a similarity matching like, Okay, if this is the sentiment, if this is the intent that the user is asking, this is most likely the answer. And then it basically decodes that, using that encoder, decoder, the transformer architecture, basically right? So no matter how sophisticated it feels under the hood, it basically does a very simple thing, which is, take your question, encode it into vector, generate the embedding, and then find the other vectors or other embedding that are similar to it to that intent. And basically that provides an answer to that, and then basically finds answer that has the highest probability of matching to the intent that you are asking, and then it takes that and then decodes that back into natural language using the same embedding. That's essentially all these models do. We have learned that now, obviously we are as individuals. We are not in a position to train these models ourselves, because it takes a lot of money, resource, right? Lot of lot of time, and lot of bright minds, right? I mean, today's AI scientists. I was looking at some job board. The starting salary starts from around 300k and that's just the starting, yeah, if you get a good master's degree or a PhD in field, right? It is not uncommon to see AI scientists making millions of dollars, right at that level. I mean, they're earning like, if you think about like NFL stars, even NFL starts will be belittled by the earning that these guys make, right? So that's the kind of mind that these companies right, like Facebook, meta, Amazon, Google, all of these companies, they throw in right and open AI also, of course, right. So these are the model builders. What we are going to learn today is how to use the. Model builders, models that the builders have built, and how to put these models into use, right? In a sense, there is one saying, like, LLM models, is the new electricity, right? So think about going back a couple of 100 years, right? So when electricity was invented, and that was a cool thing, like, Oh, you just have a socket in your house, in your wall. You just plug it in, and it can do lot of things. It can turn on a light, it can run on heat, it can do lot of different things just by plugging something into a socket. So LLM model is basically the modern version of that, right? So these LLM models are kind of commodity almost, right? And you basically have like these 5678, different large providers that provide these models that we can use almost the same way. So that's one good thing, right? I mean, it does not take a lot to basically be able to use this model. I mean, the simplest way that we have mostly used this model is just go to their website, like whether you go to Open AI's website or Google Gemini or copilot, either directly to the website or embedded into some some application like Microsoft Office does have copilot building. I mean, not just Office, Microsoft, for Microsoft, anything and everything that they are pushing out these days do have a this co pilot button at the top. You click on that, and you have a window popping up to the right side of your screen, and then you can ask the question. We have seen that in VS code as well, right? So that's one way of doing it, but what the way that we are going to do today is how we will try to see how we can do this in a more structured way, programmatically using our known Python program. The reason that that is useful is because if tomorrow, if you are in a job where you are working for a company that is building application based on AI, the sort of AI first application, as they say, that's where this kind of skills will come in handy, right? So let's say you are building a building a product recommendation app. You are building a travel booking app or travel planning app, right? So sort of like different agent based application, right? So you will basically hitting those same LLM models, like it could be Gemini or copilot or Chad, GPT or whatever, right? But you are not directly querying them through a browser or anything. Instead, you have to send, kind of similar to how we were sending the query to different APIs. Remember during the first half of the course when we learned about how to call a remote API using Python, it's kind of the same way, but you are basically sending not just structured data to those APIs. You are basically sending human language, like a normal English language, questions, and they will be providing you the response. And then you can structure these response if you want to put it into an application, integrate it with your downstream data sources. You can do that by putting some structure around these unstructured response. So these are some of the things that we are going to be learning today and tomorrow, basically. So these are the, basically the two classes where we're able to learn this, and then Thursday's class. We are just going to go over mostly, some, some of the, I said, philosophical aspect of this, like, what's the what's the outcome? Like, yes, we have as a society, we have come come a long way. But where does it make it? Where? Where do we go from here, right? Or what is the impact of the society? So we will basically ponder over some of those sort of philosophical question Thursday, which is going to be our last day of class, and I'm hoping, like starting from Thursday, probably I'll be able to give you some time to group within your within your groups, and then start formally, like brainstorming your idea what you are going to work on. So that way, even after Thursday, you will have six full days of work. And then the last week, Monday, you will be doing your presentation. Okay, cool. So today we are going to when, when we, when I say that, we are going to use a Python framework, right? So that framework is broadly or loosely speaking, it's called a Lang chain. Now, even though, here it says Lang chain is a Python framework, but it's not very well structured, well defined framework, in a way that, let's say scikit learn is a framework. I mean Lang chain is basically kind of a lot of things coming together, and then there are some core Lang chain functionalities, and then there are Lang chains, sorry, LLM specific Lang chain functionalities that are made specifically for Gemini LLM or open AI LLM or Amazon's LLM, what is that called bedrock? I think bedrock LLM, and so on right. And then there are a lot of community developed Lang chain features. That allows you to basically add some additional, I'd say, wrapper around the core LM to make the conversation with the model little bit easier to maintain and manage. Right? So that's what we are going to see. So if you go to this page here first, so here, this is the Lang chain page. So you can see here, if you look into the URL, Python, dot, Lang chain.com, right? And this is the documentation for that. But the current one that I'm in here is for Google.

Unknown Speaker  10:36  
Hang on a second.

Speaker 1  10:42  
Okay, so this one, you see that I'm on Google, but if you look into any one of these, let's say if you click on AWS, so there is a AWS specific Lang chain package, people install Lang chain AWS for Microsoft. There is a Microsoft, Microsoft, oh, Microsoft package. It's actually Lang chain, open AI, then the open AI, oh, open AI. And Microsoft, they have the same flavor of Lang chain that they use, which is Lang chain, open, AI. There is a Lang chain hogging phase. Lang chain for anthropic AWS. Also we saw and then lanchin for Google, which is for the Gemini, right? So around this core LAN chin framework, there are these different providers, and each providers kind of provide their own version of LAN chin package, which is kind of the handle that you can use to hit that particular LLM, that's what. And then when you do this, you will basically all you need to do is you basically need two things. One is you need to get your API key from the corresponding LLM provider. And then in your code, you will basically have your API key like load from an environment variable or something. And then when you create the LLM, you will basically provide the particular version of the model, and then that's your LLM from that point onward. And then you can basically send any question using the run command or invoke command. Both both works. So run, I think, was the previous version. Currently they are saying use invoke instead of run, but both works. So that's what we are going to do, starting with this launch in Google Gen AI now you have to go to Google AI studio, right? So if you go to Google AI studio, actually, I think I have here. So hang on. What happens if I click in here because that was in my I also

Unknown Speaker  12:44  
open in the live Chad, yeah,

Speaker 1  12:47  
so Google AI studio is blocked with my boot camp profile. That's why I went there with my personal profile. And if you go there, you will basically go into this AI dot, Google dot Dev. And here on the left side, you go on API keys here, okay, and then in this page will come up and it will say, get a Gemini API key in Google AI studio. So this is

Unknown Speaker  13:14  
put a link for it also already on the Live Channel.

Speaker 1  13:17  
Okay, yeah, thank you. So you click in here, and then you will basically say, create an API key. You click this button. In my case, I already have created one. So this is my API key. Here it is showing you can click Copy, and then that's the one that you put in a dot env file or put it, put it in your bash. Say, shell, like, bash, RC, like, do whatever way. Like here, for example, they're saying, Hey, you can take this API key and then not here. Here, like it says, if you are in Windows, you can basically export it as your environment variable in a Linux or Mac OS. You can put it in your bash RC, right So, and then you source the BAC. Or you can just do the simple way, like we have done before. You basically have a dot env file, and you basically put a Gemini API key right there, right so whichever way you do, then, going back to your Python file, you need to obviously have that dot env, the load dot env, and then with that, you are basically going to do a os dot, get environment variable, and get that environment variable For the Gemini API key that you said. So let's pause here and make sure that everyone has done this step. So could

Unknown Speaker  14:47  
you just walk through that one more time?

Unknown Speaker  14:50  
Ai dot Google, right? Yeah,

Speaker 1  14:53  
hang on a second. Let me go back here. Yeah. So you go to AI dot google, dot Dev and. Then you go to API keys. Obviously you have to sign in right with your Google profile. So and then on the left hand side, you click on these API keys under the Get Started section, and then you click this blue button that says, Get a Gemini API key in Google. Ai studio

Speaker 2  15:19  
was there, was there a step that we did before that account or something.

Speaker 1  15:26  
Well, as I said, right, you have to. You have to basically sign in. You see here, I did. I am signing, yeah, and

Speaker 2  15:33  
then you, then you, and then you did well, because AI Google Dev, and I'm not seeing this. You

Unknown Speaker  15:40  
are not seeing this, the API keys.

Speaker 1  15:44  
Let me see. Let me see. So if you go to AI, dot, Google, dot Dev, so are you here in this page? Yeah, okay, so let's see how we go from this solutions,

Unknown Speaker  15:58  
maybe a general API, yes,

Speaker 1  16:01  
I think it will be Oh, so here, so, yeah, click on Go, hover on solution and go to Google, AI studio. No, even I am, oh, actually, even here also you can do so then, if you do that, and then here also that your get API key button is there, yeah, you can go that way too. So or directly hit this link, AI studio.google.com,

Unknown Speaker  16:29  
directly hit this, yeah.

Unknown Speaker  16:32  
And then you should have this get API key here.

Speaker 1  16:37  
And then you create an API key. I'm not going to create one because I already have one here that is showing, this is a free plan. Oh, it actually shows uses data as well how much I have used. I don't know how much free quota that they give. Let me

Unknown Speaker  17:02  
just create a dot env file,

Unknown Speaker  17:04  
yeah,

Speaker 1  17:06  
yeah. So you do that, and then once your API is done, you copy this API key from here, and then you go into your wherever your Jupiter notebook is, and you can create a dot env file there, or,

Speaker 2  17:25  
Yes, go ahead. Jesse, I was gonna say, Are you copying the whole block? Or just,

Speaker 1  17:31  
I'm copying. I'm copying. So if you click on this, so do you have, there is a copy button here,

Unknown Speaker  17:39  
so you just copy that.

Unknown Speaker  17:44  
So it will say copied. It lets

Unknown Speaker  17:45  
you recopy the API key.

Unknown Speaker  17:50  
They like, make it secret after the first time I show you,

Unknown Speaker  17:54  
um, no, it's actually for

Unknown Speaker  17:57  
like other APIs,

Speaker 1  17:58  
no, but they're not doing that. I know some API do that, but here I have created it before, not today, and I could still go back here and click on these, and it is still giving me my full API key.

Unknown Speaker  18:09  
Yeah, I thought that was neat, interesting.

Speaker 3  18:18  
Right below the API Quick Start Guide is that a command prompt to test if it's working like you would just

Unknown Speaker  18:28  
there might be Hang On a second.

Unknown Speaker  18:35  
Where did my other pitch go? I

Unknown Speaker  18:43  
So that's,

Speaker 1  18:50  
I'm not seeing any, any such thing like test your API key, kind of what you said. I'm not seeing that here,

Speaker 3  18:58  
asking, what that like that code, that snippet code is,

Unknown Speaker  19:01  
Oh, you mean this one

Speaker 1  19:05  
you are, are you talking about this code? Yeah, yeah. So this code, they're saying. So see what we are going to do is we are going to use these API key to hit, hit their API REST endpoint, but we are not going to go directly. So what this is showing this is the Carl command. So if you want to take these and run it from your terminal, like your bash shell, or whatever terminal that you are using, this is basically showing that what actual HTTP address you are hitting to get the corresponding to get to the corresponding Gemini model, which is, in this case, is Gemini two.or flash, right? So you can do that like you basically have to go to your terminal and put this comment command there, and that way you can test. But for our purpose, what we are going to do is we will take this API key and then initiate a Python Lange object. Object, and that Python lantern object works as a wrapper around these underlying HTTP endpoint so you really don't have to hit the endpoint yourself.

Unknown Speaker  20:18  
Make sense?

Speaker 4  20:20  
Was say you didn't see where the API key goes up, goes right up to him, the curl command.

Speaker 1  20:29  
This is the curl command. So if you do that, then your API key will

Unknown Speaker  20:32  
go You said you didn't see it. That's okay.

Unknown Speaker  20:35  
And, no, no, I did see okay.

Speaker 1  20:40  
And in this this example like you are basically asking a question. Explain how AI works, so you can do that, and then it will basically give you an answer. Okay, but what I'm saying is you really don't have to do this card. Just copy your API key, head back to your VS code and create an environment where dot env file and put Gemini underscore, API underscore, key equals, and then post your paste your API key here, save the file. That's it. Oh, okay,

Speaker 2  21:16  
yeah, the Gemini underscore, API underscore, correct,

Speaker 1  21:20  
because, see, this could be anything. It doesn't have to be. All you have to do is make sure whatever the key that you are using, the name of the key you are using. The same thing when you are doing a get environment using your Python function, Python OS function, that's all. So this name here has to map this match this name here this name and then this is your personal API key that you are going to test.

Unknown Speaker  21:52  
Okay, so,

Speaker 1  22:07  
okay, so after you run the Oh, another thing you have to do, I almost forgot. So you also have to do this deep install, which you see it says launch and Google Gen AI, which is something that you saw, not here, here in this space, yeah, peep, install hyphen you basically means, like, update to the latest version. You can use hyphen you or not just pip install langen, Google, gn, AI, which is the script and command here. And then you should be able to run this code with no problem, because now the Lang chain, Google Gen AI is installed in your Python environment. And then once that is done, you do a lot load, dot, dot, env, and then you specify which Gemini model you are going to use throughout the remainder of the code, you will see in the file given it is probably given as 1.5 flash. This was an older version of model, so all of these activity, it will run with 1.5 flash. But if you want to get the experience from the latest model change the 1.5 to 2.2 I think 2.5 is also available, but I think they that that might still be something like it, but I think it is still under preview kind of thing. So I think 2.0 is a good, stable, latest release that you, you guys, can use.

Unknown Speaker  23:38  
Were you looking at a baby before that? Said? Sorry

Unknown Speaker  23:46  
there's a dot env file for the keys,

Speaker 2  23:52  
because the reason why I asked that is your first cell has underscores, but when I tried to do underscores, it errored out. I had to do dash

Unknown Speaker  24:01  
which cell you were talking about.

Speaker 2  24:07  
So if you look at your first cell,

Unknown Speaker  24:12  
are you talking about the PEEP install there? Yeah, that won't

Unknown Speaker  24:15  
work. You have to use dashes,

Unknown Speaker  24:19  
ah,

Unknown Speaker  24:21  
whatever documentation you were looking at had dashes,

Speaker 1  24:24  
Oh, I see, I see, yeah. Now hang on. But how did I do it? Let me run it again.

Unknown Speaker  24:36  
I when I tried doing it that way, it heard all,

Speaker 1  24:42  
oh, I didn't notice that. Hang on a second.

Speaker 5  24:46  
If you pick the right kernel too, you could just do the PIP installing terminal for that conda environment, and then you don't want that and that command in the No,

Speaker 1  24:55  
that's not the point. What Jesse is saying is in, in here, you. Is lag chain, hyphen, Google, hyphen, Gen AI, whereas I use lag chain, underscore, Google underscore, Gen AI,

Unknown Speaker  25:09  
are these two different packages.

Speaker 1  25:12  
They hit the same thing. You see in my case, both are saying requirement already satisfied. So, so both Lang chain and hyphen is basically pointing to the same package, Jesse,

Speaker 2  25:25  
yeah. I don't know why, but when I tried to install it more and you know what level,

Speaker 1  25:34  
yeah. And also, also look at these import statement, right? So there is an underscore there, LAN, Chan, underscore, which is why

Speaker 2  25:41  
I tried to do a pip install on that, and then we're going to air it out. I was like, wrong when I saw the hyphens. I was like, maybe that's it. But anyway,

Speaker 1  25:50  
I don't know. I am using python three. Dot 11. Dot 11.

Speaker 2  25:56  
Well, you know what? It's okay, Benoit. You know, obviously it's working. So I'm super excited. Okay,

Speaker 1  26:04  
okay, cool. So if you have your Gemini key loaded here, if you really want to see whether your Gemini key has been loaded, you can actually try to print it. If it is loaded, it will be printed, but make sure that you don't save it. So I'm deleting it, so that means my Gemini key have been loaded. Then the first thing you need to do here coming down is basically, you need to create an object that basically isn't handle your gateway to the underlying LLM, which is this class variable, Chad, Google generative, AI, and you need to provide what your API key is, which you loaded from your environment variable, and you need to provide the model version, which is Gemini two.or flash in this case, and that gives you a LLM that you can then invoke any query against. And then you can basically just do an LLM dot, invoke and do whatever. Okay, so in this case, the prompt is, give me two dinner dinners to make this week. One has to have oops, typo. One has to have beef and the other park, right? As you can see, very human like language or not human like human language. And I am not using any Lang chain as per se, because here my query is directly hitting the LLM. There is no other wrapper on top of it. And the result that comes back, the actual content is inside another field in that result object called content. So you have to print result dot content, and that basically gives you this, okay. So if you go to Gemini portal directly and you hit this question, you will probably get something similar there as well, although these are probabilistic models, so there is no guarantee that they will give you the exact same result, but something along these lines, and you can play around with it, you can put literally any question that we all ask directly to our AI these days. So this is essentially we are doing the same thing, except we are just invoking this from a Python program that's all,

Unknown Speaker  28:35  
okay.

Speaker 1  28:38  
Everyone been able to do this and get some response, yep,

Unknown Speaker  28:46  
anyone stuck somehow?

Unknown Speaker  28:49  
No, I passed it b, and it still gave me b. So smart,

Speaker 1  28:53  
okay, so it forgive you for the typo you make.

Unknown Speaker  28:59  
Okay, that's good.

Speaker 1  29:04  
And then this one, it's basically the next cell. It's basically showing just a concept, like, Hey, if you want to turn it into an app, let's say this is a stupid, rudimentary app. Let's say you have a like a app that basically asks the user like, hey, what do you like in your meal, and user says, chicken, beef, pork, fish or whatever, right? So you will basically ask you two question, and you basically put things, put the actual question answering part inside a reusable function called dinner recipe, and then you call it, which is, it's pretty simple, like a Python, like a syntactic sugar added onto it. But essentially what you are doing, as you can see inside the function, you are getting the your LLM, and then you are getting the query, in this case, the food one and food two, you are getting dynamically from the user input here, which you are passing to the function. And you are basically almost like creating a rudimentary app, like when you run this. Is so because of the first input, it is asking me, what is the ingredient you like in your food? So let me put, let's say chicken, and then the second day's meal, the second question, let's put fish and enter and you see now the query is running against the LLM, this time with chicken and fish instead of beef and pork. And here it gives me two different dinner ideas, 11 hard roasted chicken and veggies, and it gives me the ingredients instruction so pretty well written, it seems. And then the second date, saying fish she should pan salmon with asparagus and cherry tomatoes with ingredient selects, and then it also says, hey, there are some variations you can also try. So, pretty good, huh? Like, who needs cookbooks?

Unknown Speaker  30:51  
Where do you have LLM,

Speaker 2  30:53  
you know, the glean prompt library that I use at work has the ability to put in variables, like, so you can create a prompt and then, like, have variables, and so you can have people reuse your problem,

Speaker 1  31:06  
yeah. So now see these. These functionality can be expanded in many different like, you can be creative. There is basically no bound on what you can do, right? For example, one thing you can do is, instead of asking about just one thing, like beef or chicken or salmon, you can try to provide, like, some additional prompting, like, Hey, by the way, in my pantry, I have these, these, these items. Can you suggest a dinner idea that will use these items that I already have in my pantry or in my fridge, right some leftover so you can do that. I have seen a company that they were actually looking to work on an actual product, product whereby your refrigerator will basically have cameras inside, and it will actually see what you have stored. And based on that, there is a touch screen interface in front of a fridge. And then you can basically through viata screen interface, you can basically generate ask the AI to generate some dinner ideas based on what you have stored in your fridge. To do those, we have to learn something little bit more, which is called prompt engineering, which we are going to see some of those techniques in tomorrow's class actually.

Unknown Speaker  32:32  
Okay,

Speaker 1  32:34  
so this far is clear, right? So basically our Google Chad generative AI class, it's basically nothing but a Python wrapper around these, around this, what it's called, what is that? This thing? So basically around these HTTP endpoint, essentially. So when you are when here, when you are saying, Chad, Google generative API class you are creating with this API key and model, essentially, you are doing this. Okay. Now there is one other thing I omitted. Now I would like you to focus in here. So in this case, you see that I have used to attribute here the API key and the model version, but in here you see that I have added a third attribute, temperature.

Unknown Speaker  33:32  
So what does the temperature do?

Speaker 1  33:36  
So the temperature is basically a variable that controls how diverse the response of the model is going to be. The temperature takes on a value between zero to one for Gemini 1.5 model for Gemini 2.0 the range of valid temperature is between zero to two, and it can be a decimal number also. So you can do like 0.1 0.2 0.3 anything, right? So the lowest is zero and the highest is two. So now let me demonstrate this how temperature might actually affect your let's do a temperature 0.1, okay, so I'm running these fixed question whereby I'm asking the LLM to provide beef and park Dinner with a temperature 0.1,

Unknown Speaker  34:36  
so let's run it. Do

Speaker 1  34:44  
Okay, so what is that item that it printed? Sheet pan stick fajitas and slow cooked pulled pork sandwiches. Keep in mind. Okay, so. Now, if you run it another time, do you think it will generate a different response, or same sheet pan steak fajitas and well, the second one is different pork tenderloin with roasted apples and sweet potatoes. So it did have some change. Let's run it another time. I mean, these things are all stochastic. Basically, the idea is, the lower your you see, the first recipe is still the same, sheet pans, take fajitas and then pork tenderloin. So second ones keep changing, but the first one is still the same three times I ran, but I am running with a very low temperature, like 0.1 actually, let me try this with zero. Let's say which is the lowest possible temperature. So think about temperature. Is higher the temperature, the more random your model outcome should be, and the lower the temperature, the more predictable your model outcome would be. So that is the whole idea. Now that may or may not work for all kind of firms, but that is the general idea. So here, when I'm running with zero, it's still stuck with sheet pan stick fajitas and pork tenderloin with roasted apple and sweet potatoes. Let me run it again. Sheet pan stick fighters, pork tenderloin with CC now with zero both of the recipes it is generating. I tried two times it generate exact same recipe. Now I'm running it third time, hopefully it will do the same. Oh no, actually, the second one changed here, slow cooker pulled pork, but the first one is still the same. So now I'm going to go all the way to the extreme and crank the temperature up to the highest possible which is two. And this time, the variation of the response should be more. So let's see what it does. Okay, beef stir fry with noodles. Finally, it came out with fait out from fighters. So beef starfry and and what was the other one? Oh, slow cooker pulled pork sandwich, beef star fried noodles. Slow Cooker pulled pork sandwich with temperature two. Let's run it another time.

Speaker 2  37:24  
I thought you said that it would be between zero and one.

Speaker 1  37:29  
Hang on a second. Okay, so one pan Steak Fajita is different, and sheet pan lemon hard roasted pork and alloy and wheat root vegetables, very different. So the idea in general is, the higher the temperature is, more diverse your responses is going to be to your question. Jesse, the range of temperature for Gemini, 1.5 flash model here, I'm not using 1.5 I'm using 2.0 if you are using 1.5 so they're saying the range of temperature is between zero to one for 2.0 model, they have increased the range from zero to two. Got

Unknown Speaker  38:12  
it. Thank you. Sorry I missed that. Yeah.

Speaker 1  38:17  
So now think about what temperature you should ideally use. Should you use a lower range or higher range when you are sending the query?

Speaker 1  38:30  
Maybe you want us probably higher right? Well, isn't it? The right answer is, it depends, like, what you are trying to do, like, for example, this particular application, you really don't want the model to give you the exact same recipe every time you ask, so it's probably higher temperature is better, as opposed to that, if you are asking your model to do some more deterministic thing, like asking the model to solve some math problem, you'd probably need A lower temperature. So it kind of varies, depends from case to case basis, right? So, but I would say for most purpose. So the default is one, actually. So for Gemini two, dot flash model, if you don't specify temperature, the default it takes is one which is basically the mid range. So for most part, I think one is fine, unless and until you are trying to do something that requires more deterministic calculation. So if that is the case, or, let's say, later, you will see that I'm going to ask these LLM to go and fetch data from another API using certain parameters. So for those thing, I'll probably go on the lower side of the temperature, because I want the model to be able to hit another API, which could be, let's say, a Wikipedia or like anything like open trivia database or weather API or NASA API, or anything where I am. Not going to hit those APIs myself. I'm going to ask my model to go and hit those API for me and get data from there. So when you ask, when you want your model to do something like this, which is very specific in nature, you should go for lower temperature, okay. Whereas something where you want your model to be more creative, such as this, you want to go with a higher temperature. So higher temperature meaning more creative, but that means the risk of your model hallucinating can also be more the lower temperature meaning more deterministic.

Speaker 2  40:37  
So like tamer or more normal. Normal,

Unknown Speaker  40:41  
yes.

Speaker 6  40:45  
Okay, so that's that.

Speaker 1  40:52  
So now we, we really haven't actually changed anything, right? So now let, let's actually before going back to the slide. So let's do something here, right? So let's say this time, instead of asking the LLM to find some creative dinner ideas, I'm asking a hard math question. I mean hard, meaning not hard, hard, hard, basically meaning something that is very fact oriented, very deterministic in nature. I'm asking a question like, Hey, what is the sum of 18 and cube root of 27 and we know the answer is 21 because cube root of 27 is three. You add three to 18 and you get 21

Unknown Speaker  41:34  
So can I ask this question directly to the model for

Speaker 1  41:40  
state of the art model in their latest version, they are able to handle this by itself for the most part. So let me run this query directly against our LLM, which is the same LLM that we initially initialized up there, right? So I'm running this with the same Gemini two.org flash model, and let's see what it gives

Unknown Speaker  42:04  
what happened here?

Speaker 1  42:07  
Sorry, sorry, it doesn't have keys. Okay, so just do because that's not a dictionary, okay, so

Unknown Speaker  42:18  
it is able to give me the answer.

Unknown Speaker  42:23  
Uh, what is my temperature, right? Now,

Speaker 1  42:29  
temperature is two but even then it is fine, right? Actually, let me play around with this little bit.

Speaker 2  42:41  
So if you use 1.5 would it have been as accurate

Speaker 1  42:46  
in this case? Since this is a very simple math problem, it is probably not a very good use case to show.

Unknown Speaker  42:58  
Yeah. I mean, it is good.

Speaker 1  43:02  
Yeah, I don't think there would be any variation, no matter which temperature you choose. No, you see it is giving the same answer, but the way that the answer and reasoning is coming that is changing. And some of these have some kind of a weird character built in, which is basically the latex symbol. So the model is trying to build pull some latex symbol in that if you render with a latex aware front end, it will basically print a nice like a numerical thing, right? So that's what the model is trying to do now. And see, I ran it another time, it still generated the same thing, but there are some $2 sign here, and here, there are $2 sign at the end. So basically 18 plus three equal to 21 it generated it as an equation, but enclosed in $2 sign, you run it one more time, it basically didn't do anything else. It basically did No it did that, but it did that within $1 sign cube root to 27 is three. Since three is times three, times three equal to 27 it's Yeah. So it is every time it is coming up with the right answer, but the reasoning that it is using to get to the right answer is slightly different.

Speaker 2  44:17  
And is it? It's output markdown. The

Speaker 1  44:21  
output is basically, yeah, it is not a markdown. It is using latex. Now, by default, when you are rendering latex from a Jupiter notebook, it is not latex aware. So latex is basically the math markdown, right, that you can use in Yeah, to basically show like equation, fraction, power, exponent, integral, sign, and all of those things. So that's what led X is used for. So the point here is that even even if we have some specialized question, not like a general print me some dinner, I. Ideas when you have more specialized thing, the model does work, but there is a risk that for a more complicated work, the LLM might not correctly be able to get the intent of your Ask, and it might actually hallucinate and latch on to wrong result, which is not apparent given how simple our math problem is, but even if you look into the variability of the thought process that the model is showing, you can kind of imagine that if the math problem was more complicated than this, then the model there is a chance the model would have hallucinated. But since this is a math problem as a human user, you cannot afford the model to make mistake, so you need to put something to control the variability of the model. And that is where this specific man Lang chain comes in, which in this case is called the LLN map chain. And these Lang chains, if you look in here,

Unknown Speaker  46:01  
not here, sorry.

Unknown Speaker  46:04  
So this,

Speaker 1  46:09  
not that one. So this is constitutional chain document. Oh, I thought I have the LLM, map chain open somewhere, maybe I have closed that tab. Lm, LLM, math chain, let me, I must have opened this. Yeah, this one. I had opened this before. So you see up in the URL, it's still in the same Python, dot Lang, chain.com, and under the API reference, these are all the different additional chain that you can use, which is another set of wrap around, around your core LLM that basically interjects your query, intercepts your query and provide some additional structure to help the model better understand what the intent of is it? Intent is you are asking the model and also transform the model response back in a way that is more predictable, reducing the chance of making an error. So that's what the language, sorry, math. Lang chain specifically does. Now, for any Lang chain, the way that you would ideally use is so first of all, in order to do this, you to, in order to be able to do this import, you need another peep install, which is Peep install, Lang chain, this time just blank. Chain, so keep in mind that peep install we did up here. It is Lang chain, Google Gen AI. So it is Google Gen AI, specific, plan chain, wherever, however this Lang chain that we are going to be using from now onward, these are more general purpose, rank Lang chain that you can use to wrap around any other vendor specific LLM like, whether it's a Microsoft Open AI or Amazon or Google, you can use this general lancha to wrap around any other LLM that you are using. So the way to use it is these Lang chain is you take whatever lancha class that you are going to use, which, in our case, we are going to use. In this example, LLM math chain. And these LLM math chain, as you can see, there are, it is basically one of the many, many other different chains that are available. And we are going to use some other also throughout the course of the class today. But in this example, we are just going to use the Lang math Lang chain. Now how we use it? So first, we initiate our instantiate our LLM here, in the same model, same way that we did, and we chose a temperature of zero because we want the model to be as deterministic as possible. We don't We are not going to encourage random thought here. So that's one. And then we are going to create a map chain object where we are going to pass these LLM as a parameter. So basically, these map chain is a wrapper around your underlying LLM, and then the second parameter is optional, verbosity, verbose equal to true. These basically, if you make it true, it will basically show how the call is going from Lang chain to the underlying LLM and how the response is being generated, which is good for a development purpose, but it also kind of makes your response look clunky. So once you're Once you are sure that these LLM and Lang Chad combination works well for this particular use case, you should change the verbosity to false. Okay, so if you change the verb. Set it to true. And now I'm going to ask that same question here, but unlike LLM, dot invoke, I'm not doing underlying LLM invoke. I'm doing the invocation on the chain, which is the wrapper around wrapper object around the LLM. So I'm doing chain dot invoke instead of llm.in book. This is kind of similar to how we used pipeline in scikit learn. Remember, in scikit learn, if you take any model, let's say SVM model, you can do a fit and transform on the SVM model. Or if that is Vm model is part of a scikit learn pipeline, then you are you can do a fit and transform on the pipeline object itself. So think about this Lang chain as kind of a pipeline, and you can have multiple steps inside that pipeline, although in this first case, it is a very simple scenario where your pipeline just contains only one thing, which is a single LLM, which is the Gemini 2.0 model. So pipe one, pipeline wrapping around one. LLM, so that the problem to our math question becomes more dependable with less variability. So let's see what it does. Okay? So now you see also time wise, you see it took also much less time. And since we have verbosity set to true, it basically says entering new LLM match chain. And this is the question, and then it basically cuts out through all of these gibberish thing that the model was printing. It basically says, Hey, this is the question. And in order to do that, this is the specific expression, that Numeric Expression that I am going to have to evaluate. I, meaning I being the AI model, which is 18 plus 27 to the power 1/3 which is a very specific math equation that the model is going to evaluate every time. And no matter how many times you run it, it will give you the exact same thing with no variation at all,

Unknown Speaker  52:19  
because of the work that the LLM math chain is doing okay.

Speaker 4  52:28  
And the important thing to point out to manage to along the way is the way it's working, which is that it's integrating a tool, a calculator, tool that the use and it can say, Okay, do this calc. And it writes that as actually a Python expression, sends it as a string to the calculator tool, which runs it as a Python expression, and then returns the answer. So it's actually what you're doing is you're giving the LLM, may or may not be like you said, be good with doing the math, but we give it a calculator to fly. You

Speaker 1  53:01  
give it a calculator? Yeah, so kind of in a way, that we are validating. We are providing an extra layer of validation right on top of the work that the LLM is doing on our behalf. Now to try, try to prove my point, what I wrote is, I kind of thought about a slightly more difficult math problem that I wanted to ask, and this is something I wrote it you've probably not seen it in the file that you have. So here what I'm trying to do the model. I'm giving it an algebraic equation, a quadratic equation, which is very simple, x squared plus x plus one. And I'm asking the models to find an integral of this, basically the area under curve between x equal to one to x equal to two. So basically this is a definite integral problem. So now we know that the integral of this would be x cube over three plus x, square over two plus x and then you take that between two and one. So basically you sub in x equal to two, in that x cube, over three plus x squared, over two plus x, what is the value of that at x equal to two? And then you also find what is the value of that at x equal to one, and you subtract the two, and then you get the area under the curve. That's how the definite integral works. So now I want to try this query with this function and this lower limit, one and two. So I just want to run this okay.

Unknown Speaker  54:35  
Now

Speaker 1  54:38  
I'm going to run it two ways. I'm going to run it via the chain, and I'm also going to run it directly on the underlying LLM. So let me first run it directly on the underlying LLM, and I also have put a time magic command to see how long it takes. So these simple, integral command I am running against LLM. Directly. And it took me 2.8 second, and it basically went lot of things, blah, blah, blah, blah, blah, blah. And you see all of these thing. These are all latex commands built in so and then to get the final answer. It is finally giving me the final answer, which is 29 over six. And 29 over six is actually a correct answer. I have checked it because 29 over six is four point something right. Hang on a second.

Unknown Speaker  55:40  
Yeah, 29 over six is 4.833

Unknown Speaker  55:44  
so 4.833 is the correct answer.

Speaker 1  55:48  
But looking at this output, do you think this output is really any helpful here? No, right, it did the work correctly. If I ran it one more time, it will probably again create that same output. It took even longer. Wall time is 3.17 second and again, lot of gibberish, but the final answer is again, 29 over six with a bunch of latex symbols around data now. So that's how the model is behaving. It is giving me the right answer, but in a very roundabout way. Now I'm going to take the same query and run it through the chain. Yeah. So here, also, in this example, you see I said chain dot invoke. And here I'm saying chain dot run. No difference. You will get the same result.

Unknown Speaker  56:48  
Look at that.

Speaker 1  56:51  
So what it is saying? So you see, look at this function, right? So what is the integral of this function? It will be x cubed over three. So you see what is the first term, two to the power three over three, because x cube over three, at x equal to two is two to the power three over three. And then the second term integrates to x square over two, but that at point two is two to the power two over two, and then the third term integrates to x which at x equal to two is plus two. And then you are taking the same expression and providing, inputting a substituting one for x, which is this thing, and this whole thing is subtracted by this so this is your numeric expression that you are finally evaluating and coming up with an answer, 4.833 which is the exact same answer as the model directly got, which was 29.6 but here, not only it is very easy for you to see and look at the wall. Time, it took 700 millisecond, as opposed to over three seconds. If I run it again, it will give me the repeat the exact same thing, 834, millisecond, so over and over again, it will be able to give this give you the exact same answer, and at a much lower cost, because it is doing Lang chain is doing lot of additional pre work to structure the input properly before it presents it to your model,

Speaker 2  58:20  
so that pre work is, is more, is less LLM and more, maybe, like traditional techniques, is that why it's going so fast? Because you're giving like it's going through this pre processing that making sure the LLM, that's something really well

Speaker 1  58:35  
for I am not sure whether it is the traditional I think it also uses, probably some other vector embedding, maybe some model that are available, maybe somewhere in your what is that called, hugging face model, some embedding. I'm not sure exactly how it is built in, but these are all open source. So for those of you who are curious, how are they doing it? You can definitely go and look into the code, but the idea is that it basically does some additional thing. And that actually is a good point. That brings us back to this slide, that what is it? A Lang chain generally does in principle. So I'm not saying this particular example, we used all of these six steps, but in general, these are the six key functionalities that a Lang chain is supposed to provide the Lang chain framework, which conceptually think of it almost like a similar to your scikit learn pipeline framework. Now these are the six things that it provides. So it so first one is the models, which allows you to model access, model from open AI, hugging face and any other thing which we have seen. Because when we are initializing the Lang chain, we are initializing the Lang chain with a particular LLM. So your first functionality is to provide access to other. Models, which is kind of a slam dunk, yeah, we know that. And then it prompts provide prompt templates from for llms. So in order to get the job done, and this prompt, what the prompts are probably you are not maybe understanding exactly right now, we are going to see an example of prompt engineering in tomorrow's class, but llms provide a more structured way to basically ask the question in a more structured manner that makes it easy for the model to understand what your intent is. So that's the prompting part of it, and then the chains, which is basically the actual chaining. So if, let's say your your to get that job done, it may require a multiple calls to the AI, the LLM model, first call with some intent, get the result and take the result and do a follow up question and so on. So it basically facilitates that, which is basically where the name chain came from, which is a set of calls executed in sequence to combine multiple, what I say, multiple answers from LLM, or in this cases, in multiple parts of The program. Then it also allows you to provide some context. So if you are asking, let's say, three question in a sequence, and if you want to say, Hey, I asked the first question, and based on the first question, I'm going to ask the second question, but I'm not specifically asking, what is the topic of the second question that will come from the answer of the first question. So that means someone has to memorize. What is that? What was the answer for the first question? Okay, so let's say I'm asking two question. Let's say who is the president of United States? Is your first question and your second question is,

Unknown Speaker  1:01:58  
what is his age?

Speaker 1  1:02:01  
Now, these two question, if you ask it in a, in a, in isolation, the first one LLM will get the answer anyway, because most LLM will know who is the current president of the United States. But if you just ask the second question, what is his age, or what is his date of birth, anything? So here, what do you mean by his who is he? Like asked in isolation the second question, LLM would not be able to answer you. So that means, if you want to have this kind of a sequence of question, more of a conversational style, you need to have some intermittent intermediate memory to hold the context from the previous question. So that is one key functionality does LLM provide, to provide a memory like a running memory, and then the other fifth functionality is basically providing some utility that will allow you to provide your own data when you are asking question. So instead of asking a single line of question, so let's say if you want to give the model a book to read and ask, like, Hey, what is the summary of this book? Right? So you might want to give a PDF file or something, or if you want to provide, let's say you have a set of data, like a table, format like a CSV file or something, or an Excel file, and you want to provide that data table to the model and ask it to find some statistical summary from the table, like, Hey, what is the pattern in that data set, and stuff like that. So for those kind of thing, you need additional utility function to load PDF file, cdsv file, image, audio and stuff like that, to provide additional context to your question. So these are the additional utility function that llms provide. And then finally, the agent based application, whereby your application that you will be building, you don't actually have to specify what to do, but rather, you will basically tell the AI what the world looks like. And based on that, your model, not model, your application will basically take certain action. So for example, if you want to build something like, Hey, constantly monitor your Gmail inbox, and if there is a certain email from so and so sender or with so and so subject line, then invoke something. So these are the kind of thing to basically make a context our application whereby there would be intelligent agents sitting there watching the universe and based on the what is happening around it, it will basically kick into the action and and take certain action on behalf of you. So these kind of agent based application. So that's also the other functional key functionality that blank just provide. Is that agents and tools that are powered by llms underneath that will allow you to do this kind of application where you don't have to actually go and check your Gmail yourself, or your Slack channel yourself, or the latest document in the Wikipedia yourself. You want the LLM to do it, so LLM by themselves cannot do this. That's where these Lang chain with their agents and tools come in and fill in that gap to allow you to do this kind of cool application with very little effort. So these are the key six functionalities. What we have seen is basically modeling, and we haven't seen chaining or memory or anything yet. What we have seen is we have seen a modeling and this prompting here, we are not basically doing any specific prompting ourselves, but what LLM map chain is doing, it is taking your the plain vanilla question that you are asking, and it is generating specific prompt that is going to the LLM in a way that LLM is able to come up with the answer in a much more efficient manner, rather than you just asking the question in plain English language. So it is basically providing more structure to your question under the hood, and that's why in both of these math problems, we noticed that the answer is much more easy to understand. It is coming much faster and more accurate.

Unknown Speaker  1:06:41  
Okay,

Unknown Speaker  1:06:46  
is it clear? So far,

Speaker 2  1:06:51  
I think I'm just still trying to figure out, like, what it's trying to say chains, yes,

Speaker 1  1:06:57  
you will see now you will see slowly it will so in this in the next example, if you see here, you will act it will actually help you understand the chaining concept a little bit more. So in this example, what we are going to do is, instead of we asking a problem, asking a math problem. So let's say, if you go and ask a question to Gemini, like, hey, please write a simple math what? What problem? So I'm going to ask the AI itself to write a math problem for us. Instead of we writing the math problem like a random math problem, I'd ask it to generate and then I'm also going to ask the LLM to now solve the problem. So you generate the problem, and you then give me a solution. So essentially, trying to show that how a two step process will look like, and how we can change these two step using a Lang chain. So if what I'm saying is, if you take your LLM, let's Say

Unknown Speaker  1:08:05  
LLM, dot invoke

Speaker 1  1:08:10  
and basically ask this question.

Unknown Speaker  1:08:22  
And print the content of the result.

Speaker 1  1:08:27  
What is it saying? You see what problem it created. And to solve the problem, it will require a multiplication, because it says 12 cupcakes per hour, if they bake for five hours, how many cupcakes, which is 12 multiplied by five, which is 60. If I run this multiple time, it will probably make a different Well, in this case, I think we are using Aha. We are using a very low temperature. So it is basically generating the same problem every time. But you get that now I can take the output of this and do another LLM, invoke and take these output

Unknown Speaker  1:09:10  
and ask these as a question again to the LLM,

Speaker 1  1:09:18  
and then LLM basically shows its solution, and then final answer is 60. And here, as you can see, that since I have direct been asked, I have asked this question directly to the LLM, not through a Lang chain. The final answer is not very well structured. It's basically a lot of blabbering. And then finally, you scroll to the all the way to the end, and you get that little answer there. So now in this example, you will see that how we are going to chain these two together using the Lang chain. So what we do first, we generate the chat prompt template. So there are two template, two chain that we are going to use. So one is the. Is LLM chain, and then next is the LLM math chain. So there are two things that we are going to use. So the first chain, the purpose of this is to basically generate a response to a chat prompt. Because this first question here, this is a simple chat question. Hey, please write a simple math word problem with so and so criteria. This is a simple you are asking a chat bot, and the chat bot is giving you an answer. So that is the kind of interaction that Chad LLM chain handles better now these LLM chain in addition to providing what is the underlying LLM, you can also specify a particular format of prompting here, which comes from another template here, which is called the chat prompt template, and from with that chat From chat prompt template, we are generating a template with the specific query. So that is the query that we are going to generate, and that chat, simple chat prompt is passed to the first chain, which is our chat chain, and then, just like what we did up above, our second Lang chain is our LLM match chain. Now, since we have two chains, we are setting them into two different variables. The first one we are calling Chad chain. Second one we are calling math chain. And now we are going to run these two in sequence, one after the another. So what we do? We use something called a sequential chain, or simple sequential chain, to be specific. And here we provide what are the chains that we are going to run in sequence, meaning where by the answer from the first one would be used as a prompt for the second one. So Chad chain and match chain in that order, and we are setting the verbosity to true, because we really want to see what's really going on. And then finally, our query, which is, please write a simple math word problem requiring multiplication, and then you invoke the query using this, and when you do that, so essentially, there will be two calls going to LLM first to generate the random question and then solve that question.

Unknown Speaker  1:12:24  
So let's see what it prints.

Speaker 1  1:12:29  
Yeah, so entering the new, simple sequential chain, and this time, the question generated is this, and the answer is 75 finished chain. So you see how nice and simple it looks like. It is basically printing the question and the final answer. And it is abstracting the two round trip between you and the LLM. It is basically giving you very nicely formatted question and answer. So let me actually run it with a higher temperature, maybe at least the chat one. Let's run the chat with a higher temperature, and let's run the math with a lower temperature. I think that would be a better thing to do. Yeah, so since I made the temperature to two, now, second time I ran it, it is giving me a different math question. Third time I run it, it is giving me a different math question. But every time you see very easily, there is a there are basically two numbers, they have to multiply, and it is giving you the right answer every time. So this is also one example why I used a high temperature for these and low temperature for this. So think about the possible use case for this, right? So let's say this is an application that you are using to, let's say, generate random math test question for your kindergarteners. Now you want to generate 10 question kind of on the very similar question, but just changing the wording little bit to give the kids a little bit more practice on different word problems. So that's why setting the temperature high on the first Chad chain is good, but you want the math chain to be able to solve the problem correctly every time. So setting a low temperature on the math chain is a better idea, or the math. LLM, is a better idea, so that's why I chose two here and a zero here.

Speaker 1  1:14:33  
So JC, I hope this kind of helps you understand little bit better how the chaining concept works, right? Yeah,

Speaker 2  1:14:40  
it feels like the concept of prompt chaining, and I'm trying to figure out, like I was trying to mess around to see if I could do, like, two math chains in a row with a prompt.

Speaker 1  1:14:52  
Yes. I mean, as I said, I mean these things like, they are not limited by, like, whatever example that you. Are seeing, right? I mean, they are only limited by your imagination, right? So come up with the use case and just try play around with these things. There might be very hard map problem that will require two step map land chaining. Map chaining, there might be

Speaker 2  1:15:17  
one of my favorite things to do at the end of like a long prompt chain is to ask the LLM, what prompt should I have given you in the first place to get this answer?

Speaker 1  1:15:26  
Yeah, yeah. Like, think about any, let's say a hard physics problem, if you want to do a physics problem, right? So there are essentially two parts in doing hard physics problem. First, you have to take the problem and create some sort of system of equation with the problem, and that itself will need a math chain, because the problem is not simple. What problem like this? It is a physics problem, and you need to generate, like a system of two, three equation, or a differential equation, or something like that. And then you take that problem as a purely algebra, calculus problem, and then you solve that problem to get your final answer. So you can, if you have some use case like that, you can definitely do that right? Or you can look into the community Lansing, and there might be someone who already thought about these use case and you might be able to find something that basically says something like a physics land chain or something, right? So you might be able to, I'm just, I don't know where that is, whether that that is the case. But I'm just giving it as an example to to basically highlight the point that if you look into the LAN chin community package, there are like a gazillion of LAN chin that have been written by community member, right? Okay, so another one we are going to see here is something that is called constitutional chain. And this constitutional chain, I think we already saw here, actually in here. So when we are looking into the LLM math chain, remember all of this math Lang chain that we have, there was one that says constitutional chain. So this is the page for the constitutional chain. So this one, although they are saying this is currently deprecated. So basically what it does is you can actually provide a prompt as a critic prompt that they're saying so critic from basically says, like, Hey, you are asking a model. Let's say you ask the model to, say, generate a math problem, random word problem, and they generated the word problem. And the word problem can contain anything, but let's say the students that you are going to give this problem to solve, they're kind of sensitive, and they don't want anything like any kind of a but I'd say maybe they are fearful of something. They are fearful of water. They are fearful of something. So you want to say, hey, there can be no mention of water, or there can be no mention of guns, things like that, right? Or there would be, there should not be no obscene language or obscene word that should be there in your response. So these are some additional criteria or governance principle you want to apply to your output of your LLM, so when that is the case, that's where this constitutional Lang chain comes in play, and the way it works is you basically have to create something called a constitutional principle. So this constitutional principle has two specific more important part, one is a critic request, and the second is a revision request. The critic request basically says, will say again in plain English language. So how effective your constitutional principle would be is depending on how you are writing these critic and revision request, because those critic and revision request is something you are writing, so you need to be good at writing this which only comes with practice. So you practice more, and you kind of see, okay, what kind of things work better or not. So in this particular case, the critic request is the model should not include dogs in its response or in the stories it writes. And if there is a dog, let's say if the critic request fail, then what is the corrective action the model should take, or LLM should take, that is specified as your revision request, which is modify the story to be about animals other than dogs. So think about like, let's say, for your math problem generator, you could say the math problem should not include any reference to guns, and if it is, then you can say, modify the problem to replace any reference to guns. With reference to toils, let's say same thing. So here, what we are going to do is we are basically going to ask the model to simply generate a story, which is very similar to how we are asking the model to generate a math problem in the previous example. Here, we are going to ask the model to generate a story, and that story should involve three character which are three household pets, okay? But then whoever is going to read the story for some reason, let's say the person has a fear of dogs, which, by the way, I'm adding as a name of the constitutional principle, which you should include. But that is not that important. The most important thing is your critic and revision request. But having a name also helps the model, I suppose, in some way, in clearly, like a verbalizing like or summarizing what the what is the constitutional principle used for, which is fear of dogs. And now what we are going to do is with this constitutional principle. Here this one, we are basically creating a constitutional Lang chain. And here we are providing our first chat chain that we have with this LLM as our Chad chain, and then we are providing one or more principles. So in this case, we have only one principle, which is, if there is a dog, you have to replace the dog with something else. That's your first principle. But you can have multiple such restriction, like, if you are asking us, asking the model to write a story. Maybe there are some sensor characteristic that you need to want your model to avoid, such as violence, nudity, whatever, something right, anti semitism and stuff like that. So you want to basically filter with multiple different principles that you have. So that's why, in constitutional principle, they allow you to add more than one principle, and that's why you are providing this as a list, even though we have, in this case, only one principle, which is fear of dogs, but you can have more than one principle, and then obviously you will provide what is the core underlying LLM that you are going to send it to, which is this LLM And verbosity true, just for us to see how the call really goes through. And then you take that constitutional chain and invoke that with this query, which is, give me the main events of a story with three household pets, and let's see what pet story it generates.

Unknown Speaker  1:22:38  
So entering new constitutional chain,

Speaker 1  1:22:42  
and it wrote the story. Here is a possible outline of the main events in a story about three household pets. Now, what are the what are the characters here? So the first one is Persian cat. The second one, it came up with Jack Russell Terrier, which is a dog,

Unknown Speaker  1:23:03  
and the third one is hamster.

Speaker 1  1:23:07  
So you see this first attempt that LLM generated, the story actually violated the our constitutional principle, which is the model should not include dog in stories. But then that's not the end. If you so this, if you expand this, you will see here, so, finished chain, right? So this is our first version of the story. Yeah, finish chain. And then, yeah. So then it is okay. So then it repeated the same story again, starting from here. So this is our second part. So the first part, I don't know why the first one is coming in yellow, and second one is just plain color. It should be, it should have been in a different color. But anyway, so you see what it did is it retained the first character, which is the Persian cat. The second character, which was Jack Russell Terrier named Penny, has been replaced by a ferret named Cecilia. And the third character is Sheldon, our same hamster, who is always anxious. So it wrote the story, and then the constitutional chain invoked, and then it changed the story about three change the story to basically replace that one character, that was dog, with another pet. So that shows one other example how the chaining works, right? So, two very different example, this one and the other one, which is where you are generating a math problem and then solving the math problem. But one common thread between both of these examples is both of these cases you basically had. Calls to LLM going in, so in the math problem, first call goes in, gets the math problem. Second Call goes in, solves the math problem. In this one, first call goes in, generates a story. Second Call goes in, replaces the character in the story. Now, can you do it without Lang chain? Possibly, maybe including that thing in your prompt in the first place? Maybe, if you just write these as a hey, please give me the main events of a story about three household pets. Oh, and by the way, make sure that you don't have any dogs. Maybe you can just write a prompt like that and directly send it to one Lang chain. Sorry, one directly send it to the underlying LLM. It might still work. Give it a shot. But the point here is to show that these are the cases where you should think about using Lang chain instead of sending the query directly to the underlying LLM. And by the way, this thing keeps evolve all the time, like maybe almost every day, the capabilities of the underlying LLM keeps evolving. So all of these example that I'm running today, if you run it, maybe a week or a month from now, they will probably be behaving differently and showing kind of different personality the models the underlying LLM model. So so don't, don't basically get too hung up on what particular output it is producing, because, depending on when you are running, and also depending on whether you are running 1.1 dot five, two.or 2.5 or whether you are running these On Gemini, which we are, or whether your core LLM itself is open AI or co pilot. Based on that, your mileage will also vary, right? Because each of these llms, they are, they are kind of similar, but they also have their own unique way of responding to human Question, so which we all know? So

Unknown Speaker  1:27:06  
cool question.

Speaker 2  1:27:22  
We writing this. Initially, I couldn't get it to stop, giving me a golden retriever with a puppy, but I upped the temperature to two. And get a lot better

Unknown Speaker  1:27:31  
on which one, on which example you were saying,

Speaker 2  1:27:33  
on this last example. Okay, we did the chaining. I had a temperature of 0.3 and it was just, it was absolute trash. It just kept on saying golden retriever. And then I changed the critique to say especially Golden Retrievers. And then it gave me a pod which,

Speaker 1  1:27:48  
which Gemini model version you used was the two? Oh, right, yeah, mine was two. Oh. But what did you use? 2020?

Speaker 6  1:28:01  
But when I ran it,

Speaker 1  1:28:06  
well, I ran it with two oh and 0.3 it worked. I mean, again, as I said, that's why I said, don't get too hung up on what your experience versus someone else's experience, or my experience, like everyone will run these. I mean, you will probably have different experience. But yeah, now, I mean, the thing is, if you run it is jack it up to higher, like two.or anything higher, you will basically get the probably the same behavior is just that, when you are running it multiple, multiple times, your variability of the of the response will be more so it says Persian cat, golden retriever hamster, which it then changed to Persian cat.

Speaker 2  1:28:54  
Yeah, this is what I kept getting, golden retriever. So I was like, What the?

Speaker 1  1:29:01  
Oh, okay, I see. And then what did you do? Yeah,

Speaker 2  1:29:04  
well, I have the temperature to two, but I now you're making me question that, because that just is variability. It's not accuracy.

Unknown Speaker  1:29:14  
Well, yeah, so here I see,

Speaker 1  1:29:18  
even after the this thing, it really didn't change. It was a golden retriever, and it is a golden retriever.

Speaker 2  1:29:26  
And then in the critique, I said the model should not include dogs and stories and writes, especially golden retrievers and then a bit of public and

Unknown Speaker  1:29:35  
then it did what a puppy,

Unknown Speaker  1:29:38  
a puppy, I

Speaker 2  1:29:45  
Okay, I rerun it with a temperature of two, and I got the golden issue. Again, it's just Golden

Speaker 1  1:29:52  
Retriever puppy and gold face, and let's see what this one did. Partial. At Oh no, see now it replaced the second one as a with a parrot, and then the third one is a goldfish. Anyway, okay, the second one is again as a student activity. It's the same thing where you are being asked to use a constitutional principle to do things is like, generate a meal idea with two ingredient and then you have to apply some kind of a dietary restriction. And then, based on that, you would like to see how the recipes are tweaked to match your dietary restriction. So the code is exactly same. So first you take two food name of two foods, and then you also have a requirement, which is vegan, omnivorous, kosher, or whatever you want. And then you, this is your core LLM. And then, first, you are using the chat prompt template with the LLM chain, and that you are basically calling it a recipe chain. And this is the chain you will invoke first. And then you have a constitutional principle where your critic request is that the model should only offer recipes that fit, let's say, a vegan diet, or whatever the requirement is. So this requirement is a variable here which is being replaced here with an F string, and then modify the recipes to fit that particular type of diet. So that's your constitutional principle. And then you are going to create a chain where your first entry point would be your recipe chain where the question is being asked, and then the any answer that is coming up come coming back, will be further modified with using, by using the constitutional principle, and then you will get the result. Now what I did is purposefully, I chose two full food ingredients that are not vegan. So let's say I am using chicken as my first ingredient for the first day's milk and the second day's milk. I'm saying pork right. And now for the requirement, I am going to say,

Transcribed by https://otter.ai
